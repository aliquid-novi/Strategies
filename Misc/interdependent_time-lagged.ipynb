{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8ce075-9ace-40cf-abac-70a36f932a2a",
   "metadata": {},
   "source": [
    "Types of methods to be used:\n",
    "1. Cross-Correlation Analysis\n",
    "2. Granger Causality Test\n",
    "3. VAR Model\n",
    "4. Impulse Response Analysis\n",
    "5. Lagged Regression Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a52e893-d01c-4d1c-82e7-a48e895fe67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MetaTrader5 as mt5\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mt5.initialize()\n",
    "account=51127988\n",
    "password=\"Aar2frM7\"\n",
    "server = 'ICMarkets-Demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "cd296808-b11e-4803-878e-5f57865d4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rates(pair1, tf, x):\n",
    "    pair1 = pd.DataFrame(mt5.copy_rates_from_pos(pair1, tf, 0, x))\n",
    "    pair1['time'] = pd.to_datetime(pair1['time'], unit = 's')\n",
    "    pair1 = pair1.set_index(pair1['time'])\n",
    "    pair1 = pair1.drop(columns = ['time','tick_volume', 'spread', 'real_volume'])\n",
    "    return pair1\n",
    "def corr_measure():\n",
    "    if pearson_corr[0][1] > abs(0.5):\n",
    "        print(f'Lag {i} shows a moderate correlation')\n",
    "    else:\n",
    "        print(f'No Correlation at {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a78e6e7-6882-434f-893a-3dd5fb5b2d35",
   "metadata": {},
   "source": [
    "## Hypothesis \n",
    "\n",
    "The movements of the EURUSD and GBPUSD exchange rates can be used to predict the future direction of the USDJPY exchange rate. Specifically, if both EURUSD and GBPUSD experience an increase within a given time period (e.g., within one hour), there is a statistically significant probability that the USDJPY exchange rate will decrease in the subsequent time period.\n",
    "\n",
    "## Method\n",
    "\n",
    "Gather hourly data of EURUSD and GBPUD. Process the data so that the remaining data are EURUSD and GBPUSD candles that are correlated and have moved more than 10 pips.\n",
    "\n",
    "Gather USDJPY hourly candles for each time period succeeding the new data points gathered.\n",
    "\n",
    "Run a Pearson correlation analysis between the EURUSD-GBPUSD amended data and USDJPY data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "7fb3a4fb-b88c-46ba-b369-5ca1bfc3b223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EU</th>\n",
       "      <th>GU</th>\n",
       "      <th>Spread</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-15 19:00:00</th>\n",
       "      <td>-35.4</td>\n",
       "      <td>-21.1</td>\n",
       "      <td>-14.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-15 22:00:00</th>\n",
       "      <td>-25.5</td>\n",
       "      <td>-20.9</td>\n",
       "      <td>-4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-16 16:00:00</th>\n",
       "      <td>-18.0</td>\n",
       "      <td>-33.8</td>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-16 21:00:00</th>\n",
       "      <td>-10.7</td>\n",
       "      <td>-10.2</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-17 05:00:00</th>\n",
       "      <td>-33.2</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>-16.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-20 12:00:00</th>\n",
       "      <td>-12.8</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-20 19:00:00</th>\n",
       "      <td>15.2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-21 11:00:00</th>\n",
       "      <td>12.2</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>28.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-21 17:00:00</th>\n",
       "      <td>28.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>17.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-21 19:00:00</th>\n",
       "      <td>20.2</td>\n",
       "      <td>21.1</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2097 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       EU    GU  Spread\n",
       "time                                   \n",
       "2021-11-15 19:00:00 -35.4 -21.1   -14.3\n",
       "2021-11-15 22:00:00 -25.5 -20.9    -4.6\n",
       "2021-11-16 16:00:00 -18.0 -33.8    15.8\n",
       "2021-11-16 21:00:00 -10.7 -10.2    -0.5\n",
       "2021-11-17 05:00:00 -33.2 -17.0   -16.2\n",
       "...                   ...   ...     ...\n",
       "2023-06-20 12:00:00 -12.8 -17.0     4.2\n",
       "2023-06-20 19:00:00  15.2  26.0   -10.8\n",
       "2023-06-21 11:00:00  12.2 -16.0    28.2\n",
       "2023-06-21 17:00:00  28.0  10.7    17.3\n",
       "2023-06-21 19:00:00  20.2  21.1    -0.9\n",
       "\n",
       "[2097 rows x 3 columns]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EU = get_rates('EURUSD.a', mt5.TIMEFRAME_H1, 10000)\n",
    "GU = get_rates('GBPUSD.a', mt5.TIMEFRAME_H1, 10000)\n",
    "EU_returns = pd.DataFrame(10000 * (EU['close'] - EU['open']))\n",
    "EU_returns.rename(columns = {0:'EU'}, inplace = True)\n",
    "GU_returns = pd.DataFrame(10000 * (GU['close'] - GU['open']))\n",
    "GU_returns.rename(columns = {0:'GU'}, inplace = True)\n",
    "# GU_returns.rename('GU', inplace = True)\n",
    "EU_returns = EU_returns[abs(EU_returns) > 10]\n",
    "GU_returns = GU_returns[abs(GU_returns) > 10]\n",
    "\n",
    "EU_GU = pd.concat([EU_returns, GU_returns], axis = 1)\n",
    "EU_GU = EU_GU.dropna()\n",
    "EU_GU = EU_GU.rename(columns={0: 'EURUSD', 0: 'GBPUSD'})\n",
    "EU_GU[\"Spread\"] = EU_GU['EU'] - EU_GU['GU']\n",
    "EU_GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "962a018c-f68b-4ec2-b2b9-3b64859e9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UJ = get_rates('USDJPY.a', mt5.TIMEFRAME_H1, 10000)\n",
    "UJ_returns = pd.DataFrame(UJ['close'] - UJ['open'])\n",
    "UJ_returns.rename(columns = {0:'UJ'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "debd242e-bdce-47a9-b5fd-418838227f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Correlation at 1\n",
      "No Correlation at 1\n",
      "No Correlation at 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2090 and the array at index 1 has size 624",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[335], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Keep only the rows in UJ_returns that match the index in df1\u001b[39;00m\n\u001b[0;32m      6\u001b[0m UJ_returns \u001b[38;5;241m=\u001b[39m UJ_returns[UJ_returns\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misin(EU_GU\u001b[38;5;241m.\u001b[39mindex)]\n\u001b[1;32m----> 8\u001b[0m pearson_corr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrcoef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEU_GU\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtail\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2090\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUJ_returns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUJ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtail\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2090\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m corr_measure()\n\u001b[0;32m     10\u001b[0m pearson_corr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcorrcoef(UJ_returns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUJ\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m2090\u001b[39m), EU_GU[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGU\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m2090\u001b[39m))\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mcorrcoef\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2846\u001b[0m, in \u001b[0;36mcorrcoef\u001b[1;34m(x, y, rowvar, bias, ddof, dtype)\u001b[0m\n\u001b[0;32m   2842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;129;01mor\u001b[39;00m ddof \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue:\n\u001b[0;32m   2843\u001b[0m     \u001b[38;5;66;03m# 2015-03-15, 1.10\u001b[39;00m\n\u001b[0;32m   2844\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias and ddof have no effect and are deprecated\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2845\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m-> 2846\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mcov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2847\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2848\u001b[0m     d \u001b[38;5;241m=\u001b[39m diag(c)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mcov\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2640\u001b[0m, in \u001b[0;36mcov\u001b[1;34m(m, y, rowvar, bias, ddof, fweights, aweights, dtype)\u001b[0m\n\u001b[0;32m   2638\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rowvar \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2639\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m-> 2640\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ddof \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2090 and the array at index 1 has size 624"
     ]
    }
   ],
   "source": [
    "for i in range(1, 24):\n",
    "    # Shift the index in df2 back by one hour\n",
    "    UJ_returns.index = UJ_returns.index - pd.Timedelta(hours=i)\n",
    "\n",
    "    # Keep only the rows in UJ_returns that match the index in df1\n",
    "    UJ_returns = UJ_returns[UJ_returns.index.isin(EU_GU.index)]\n",
    "    \n",
    "    pearson_corr = np.corrcoef(EU_GU['EU'].tail(2090), UJ_returns['UJ'].tail(2090))\n",
    "    corr_measure()\n",
    "    pearson_corr = np.corrcoef(UJ_returns['UJ'].tail(2090), EU_GU['GU'].tail(2090))\n",
    "    corr_measure()\n",
    "    pearson_corr = np.corrcoef(EU_GU['EU'].tail(2090), UJ_returns['UJ'].tail(2090))\n",
    "    corr_measure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "a9d56700-9192-40fe-985f-ea4d54a11402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UJ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-15 19:00:00</th>\n",
       "      <td>-0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-15 22:00:00</th>\n",
       "      <td>-0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-16 16:00:00</th>\n",
       "      <td>-0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-16 21:00:00</th>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-17 05:00:00</th>\n",
       "      <td>-0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-20 12:00:00</th>\n",
       "      <td>-0.221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-20 19:00:00</th>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-21 11:00:00</th>\n",
       "      <td>-0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-21 17:00:00</th>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-21 19:00:00</th>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2096 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        UJ\n",
       "time                      \n",
       "2021-11-15 19:00:00 -0.008\n",
       "2021-11-15 22:00:00 -0.029\n",
       "2021-11-16 16:00:00 -0.036\n",
       "2021-11-16 21:00:00  0.130\n",
       "2021-11-17 05:00:00 -0.029\n",
       "...                    ...\n",
       "2023-06-20 12:00:00 -0.221\n",
       "2023-06-20 19:00:00  0.042\n",
       "2023-06-21 11:00:00 -0.060\n",
       "2023-06-21 17:00:00  0.031\n",
       "2023-06-21 19:00:00 -0.083\n",
       "\n",
       "[2096 rows x 1 columns]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shift the index in df2 back by one hour\n",
    "UJ_returns.index = UJ_returns.index - pd.Timedelta(hours=1)\n",
    "\n",
    "# Keep only the rows in UJ_returns that match the index in df1\n",
    "UJ_returns = UJ_returns[UJ_returns.index.isin(EU_GU.index)]\n",
    "UJ_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "4dc4cc1d-6363-41f9-8627-0d820970fca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.00681756],\n",
       "       [-0.00681756,  1.        ]])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_corr = np.corrcoef(EU_GU['EU'].tail(2090), UJ_returns['UJ'].tail(2090))\n",
    "pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "fa9726b8-2321-4b47-8000-e8185a8b3969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -4.01296288e-05],\n",
       "       [-4.01296288e-05,  1.00000000e+00]])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_corr = np.corrcoef(UJ_returns['UJ'].tail(2090), EU_GU['GU'].tail(2090))\n",
    "pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "5e86f92f-3f32-4adf-8257-356ed72ec978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.00965591],\n",
       "       [-0.00965591,  1.        ]])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_corr = np.corrcoef(EU_GU['Spread'].tail(2090), UJ_returns['UJ'].tail(2090))\n",
    "pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "eb31ab93-b5ba-4993-8954-364f32b6bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Correlation at 1\n",
      "No Correlation at 1\n",
      "No Correlation at 1\n",
      "No Correlation at 2\n",
      "No Correlation at 2\n",
      "No Correlation at 2\n",
      "No Correlation at 3\n",
      "No Correlation at 3\n",
      "No Correlation at 3\n",
      "No Correlation at 4\n",
      "No Correlation at 4\n",
      "No Correlation at 4\n",
      "No Correlation at 5\n",
      "No Correlation at 5\n",
      "No Correlation at 5\n",
      "No Correlation at 6\n",
      "No Correlation at 6\n",
      "No Correlation at 6\n",
      "No Correlation at 7\n",
      "No Correlation at 7\n",
      "No Correlation at 7\n",
      "No Correlation at 8\n",
      "No Correlation at 8\n",
      "No Correlation at 8\n",
      "No Correlation at 9\n",
      "No Correlation at 9\n",
      "No Correlation at 9\n",
      "No Correlation at 10\n",
      "No Correlation at 10\n",
      "No Correlation at 10\n",
      "No Correlation at 11\n",
      "No Correlation at 11\n",
      "No Correlation at 11\n",
      "No Correlation at 12\n",
      "No Correlation at 12\n",
      "No Correlation at 12\n",
      "No Correlation at 13\n",
      "No Correlation at 13\n",
      "No Correlation at 13\n",
      "No Correlation at 14\n",
      "No Correlation at 14\n",
      "No Correlation at 14\n",
      "No Correlation at 15\n",
      "No Correlation at 15\n",
      "No Correlation at 15\n",
      "No Correlation at 16\n",
      "No Correlation at 16\n",
      "No Correlation at 16\n",
      "No Correlation at 17\n",
      "No Correlation at 17\n",
      "No Correlation at 17\n",
      "No Correlation at 18\n",
      "No Correlation at 18\n",
      "No Correlation at 18\n",
      "No Correlation at 19\n",
      "No Correlation at 19\n",
      "No Correlation at 19\n",
      "No Correlation at 20\n",
      "No Correlation at 20\n",
      "No Correlation at 20\n",
      "No Correlation at 21\n",
      "No Correlation at 21\n",
      "No Correlation at 21\n",
      "No Correlation at 22\n",
      "No Correlation at 22\n",
      "No Correlation at 22\n",
      "No Correlation at 23\n",
      "No Correlation at 23\n",
      "No Correlation at 23\n"
     ]
    }
   ],
   "source": [
    "def corr_measure(pearson_corr, i):\n",
    "    if pearson_corr > abs(0.5):\n",
    "        print(f'Lag {i} shows a moderate correlation')\n",
    "    else:\n",
    "        print(f'No Correlation at {i}')\n",
    "\n",
    "for i in range(1, 24):\n",
    "    # Make a copy of the original dataframe\n",
    "    UJ_returns_copy = UJ_returns.copy()\n",
    "\n",
    "    # Shift the index in the copy back by one hour\n",
    "    UJ_returns_copy.index = UJ_returns_copy.index - pd.Timedelta(hours=i)\n",
    "\n",
    "    # Keep only the rows in UJ_returns_copy that match the index in EU_GU\n",
    "    UJ_returns_copy = UJ_returns_copy[UJ_returns_copy.index.isin(EU_GU.index)]\n",
    "    \n",
    "    min_length = min(len(EU_GU), len(UJ_returns_copy))\n",
    "\n",
    "    pearson_corr = np.corrcoef(EU_GU['EU'].tail(min_length), UJ_returns_copy['UJ'].tail(min_length))[0, 1]\n",
    "    corr_measure(pearson_corr, i)\n",
    "    \n",
    "    pearson_corr = np.corrcoef(UJ_returns_copy['UJ'].tail(min_length), EU_GU['GU'].tail(min_length))[0, 1]\n",
    "    corr_measure(pearson_corr, i)\n",
    "    \n",
    "    pearson_corr = np.corrcoef(EU_GU['EU'].tail(min_length), UJ_returns_copy['UJ'].tail(min_length))[0, 1]\n",
    "    corr_measure(pearson_corr, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63d7f3-d97a-4125-a86b-b18a6887cbaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hypothesis \n",
    "Currency prices whose sessions open prior to other sessions can be a leading indicator of prices for currencies sessions not yet opened, which can be found through interdependent and time lagged analysis. For example, if AUDUSD returns positive in the first hour of the Syd open, USDJPY is expected to return positive too. \n",
    "\n",
    "## Method\n",
    "\n",
    "Gather data on the first 5min to 60min bars of each open, starting with Sydney. Will base currencies against the USD Markets open in the following order:\n",
    "GMT Time (MT5 is GMT +3)\n",
    "- Sydney: 9PM / 21:00 (AUDUSD)\n",
    "- Tokyo: 12AM / 00:00 (USDJPY)\n",
    "- London: 7AM / 7:00(GBPUSD)\n",
    "- New York: 1PM / 13:00 (EURUSD)\n",
    "\n",
    "Run a Pearson Correlation and a Granger Causality method on the gathered data and the respective currencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4c59e43c-3770-4023-aff5-be6e373f14b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def corr_tool(symbol1, symbol2, timeframe, bars):\n",
    "    print(f'Starting Pearson Correlation Analysis for {symbol1}, {symbol2} at {timeframe} for {bars} bars.')\n",
    "\n",
    "    pair1 = get_rates(symbol1, timeframe, bars)\n",
    "    pair2 = get_rates(symbol2,  timeframe, bars)\n",
    "    \n",
    "    s1 = input('Start Time for X?')\n",
    "    s2 = input('End time for X?')\n",
    "\n",
    "    e1 = input('Start time for Y?')\n",
    "    e2 = input('End time for Y?')\n",
    "\n",
    "    print('Times to investigate are:'\n",
    "          f'\\n{s1} to {s2} and {e1} to {e2}')\n",
    "    \n",
    "    pair1_returns = pair1['open'] - pair1['close']\n",
    "    pair2_returns = pair2['open'] - pair2['close']\n",
    "\n",
    "    lst = []\n",
    "\n",
    "    un_corr = []\n",
    "\n",
    "    for i in range(50):\n",
    "        pearson_corr = np.corrcoef(syd_opens['Syd Pip Return'].tail(i), tokyo_opens['Tko Pip Return'].tail(i))\n",
    "        if pearson_corr[0][1] > 0.5:\n",
    "            # print(f'Pearson correlation {pearson_corr[0][1]} on lag {i}')\n",
    "            lst.append(i)\n",
    "        else:\n",
    "            un_corr.append(i)\n",
    "            \n",
    "    print(f'Lags that have a correlation higher than 0.5 are:'\n",
    "        f'\\n{lst}')\n",
    "    print(f'Total Lags: {len(lst)}')\n",
    "\n",
    "    import datetime\n",
    "\n",
    "    day_counts = {}\n",
    "\n",
    "    for i in lst:\n",
    "        date = syd_opens.index[i]\n",
    "        day_name = date.day_name()\n",
    "\n",
    "        if day_name not in day_counts:\n",
    "            day_counts[day_name] = 0\n",
    "\n",
    "        day_counts[day_name] += 1\n",
    "    print(f'Days that have correlations between AUDUSD and USDJPY of more than 0.5 are:')\n",
    "\n",
    "    for day, count in day_counts.items():\n",
    "        print(f'{day}: {count}')\n",
    "        \n",
    "    day_counts = {}\n",
    "\n",
    "    for i in un_corr:\n",
    "        date = syd_opens.index[i]\n",
    "        day_name = date.day_name()\n",
    "\n",
    "        if day_name not in day_counts:\n",
    "            day_counts[day_name] = 0\n",
    "\n",
    "        day_counts[day_name] += 1\n",
    "    print(f'Days that do not have correlations between AUDUSD and USDJPY of more than 0.5 are:')\n",
    "\n",
    "    for day, count in day_counts.items():\n",
    "        print(f'{day}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78722e7d-3e80-48c9-90ed-c42c68674d14",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pearson Correlation Analysis for AUDUSD.a, AUDJPY.a at 16385 for 50000 bars.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Start Time for AUDUSD.a? 19:00\n",
      "End time for AUDUSD.a? 19:59\n",
      "Start time for AUDJPY.a? 20:00\n",
      "End time for AUDJPY.a 20:59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times to investigate are:\n",
      "19:00 to 19:59 and 20:00 to 20:59\n",
      "Lags that have a correlation higher than 0.5 are:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "Total Lags: 50\n",
      "Days that have correlations between AUDUSD.a and AUDJPY.aof more than 0.5 are:\n",
      "Monday: 10\n",
      "Tuesday: 10\n",
      "Wednesday: 10\n",
      "Thursday: 10\n",
      "Friday: 10\n",
      "Days that do not have correlations between AUDUSD.a and AUDJPY.a of more than 0.5 are:\n"
     ]
    }
   ],
   "source": [
    "corr_tool('AUDUSD.a', 'AUDJPY.a', mt5.TIMEFRAME_H1, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "48f8e48a-17dd-432d-ac69-dadfbe20019c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.50440698],\n",
       "       [0.50440698, 1.        ]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with AUDUSD, Hourly Timeframe \n",
    "AUDUSD = get_rates('AUDUSD.a', mt5.TIMEFRAME_H1, 50000)\n",
    "syd_opens = AUDUSD.between_time('19:00','19:59')\n",
    "syd_opens['returns'] = syd_opens['close'] - syd_opens['open']\n",
    "USDJPY = get_rates('USDJPY.a', mt5.TIMEFRAME_H1, 50000)\n",
    "tokyo_opens = USDJPY.between_time('20:00', '20:59')\n",
    "tokyo_opens['returns'] = tokyo_opens['close'] - tokyo_opens['open']\n",
    "tail = 50 \n",
    "pearson_corr = np.corrcoef(syd_opens['returns'].tail(tail), tokyo_opens['returns'].tail(tail)) \n",
    "pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "98d04fa7-3e60-4c26-b623-ddbf204f452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_mini_corr(s1, s2, bars = 10000):\n",
    "    \n",
    "    \n",
    "    ts1_1 = input(f'Start Time for {s1}?')\n",
    "    ts1_2 = input(f'End time for {s1}?')\n",
    "\n",
    "    ts2_1 = input(f'Start time for {s2}?')\n",
    "    ts2_2 = input(f'End time for {s2}?')\n",
    "    \n",
    "    p1 = get_rates(s1, mt5.TIMEFRAME_H1, 50000)\n",
    "    p1 = p1.between_time(ts1_1, ts1_2)\n",
    "    p1['returns'] = p1['close'] - p1['open']\n",
    "    p2 = get_rates(s2, mt5.TIMEFRAME_H1, 50000)\n",
    "    p2 = p2.between_time(ts2_1, ts2_2)\n",
    "    p2['returns'] = p2['close'] - p2['open']\n",
    "    \n",
    "    pearson_corr = np.corrcoef(p1['returns'].tail(tail), p2['returns'].tail(tail)) \n",
    "    return pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "67481bde-d6ac-4c94-bf34-de1d3e0d758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Start Time for AUDUSD.a? 10:00\n",
      "End time for AUDUSD.a? 10:59\n",
      "Start time for USDJPY.a? 13:00\n",
      "End time for USDJPY.a? 13:59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.07723649],\n",
       "       [0.07723649, 1.        ]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_mini_corr('AUDUSD.a', 'USDJPY.a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565c3b5-8129-4d87-b3b6-8fb804bba7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def hour_mini_corr(s1, s2, bars = 100):\n",
    "\n",
    "    correlations = {}  # Store correlations here, keyed by time string\n",
    "\n",
    "    for h in range(24):  # For each hour of the day\n",
    "        ts1_1 = datetime.strptime(f'{h}:00', '%H:%M').time()\n",
    "        ts1_2 = (datetime.combine(datetime.today(), ts1_1) + timedelta(hours=1)).time()\n",
    "\n",
    "        ts2_1 = (datetime.combine(datetime.today(), ts1_1) + timedelta(hours=1)).time()\n",
    "        ts2_2 = (datetime.combine(datetime.today(), ts2_1) + timedelta(hours=1)).time()\n",
    "\n",
    "        # Convert to strings for use in function\n",
    "        ts1_1, ts1_2, ts2_1, ts2_2 = [t.strftime('%H:%M') for t in [ts1_1, ts1_2, ts2_1, ts2_2]]\n",
    "\n",
    "        p1 = get_rates(s1, mt5.TIMEFRAME_H1, 50000)\n",
    "        p1 = p1.between_time(ts1_1, ts1_2)\n",
    "        p1['returns'] = p1['close'] - p1['open']\n",
    "        p2 = get_rates(s2, mt5.TIMEFRAME_H1, 50000)\n",
    "        p2 = p2.between_time(ts2_1, ts2_2)\n",
    "        p2['returns'] = p2['close'] - p2['open']\n",
    "\n",
    "        pearson_corr = np.corrcoef(p1['returns'].tail(bars), p2['returns'].tail(bars)) \n",
    "        correlations[f'{ts1_1}-{ts2_1}'] = pearson_corr[0, 1]  # Store correlation\n",
    "\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1cc76eec-4607-44f4-9575-e1073f725eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def mini_corr(s1, s2, bars = 500):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for offset in range(24):\n",
    "        for h in range(24):\n",
    "            \n",
    "            # start and end times for first time series\n",
    "            ts1_1 = timedelta(hours=h).seconds // 3600\n",
    "            ts1_2 = (ts1_1 + 1) % 24\n",
    "            \n",
    "            # offset hours for second time series\n",
    "            ts2_1 = (ts1_1 + 1 + offset) % 24\n",
    "            ts2_2 = (ts2_1 + 1) % 24\n",
    "            \n",
    "            p1 = get_rates(s1, mt5.TIMEFRAME_H1, 50000)\n",
    "            p1 = p1.between_time(f'{ts1_1:02d}:00', f'{ts1_2:02d}:00')\n",
    "            p1['returns'] = p1['close'] - p1['open']\n",
    "\n",
    "            p2 = get_rates(s2, mt5.TIMEFRAME_H1, 50000)\n",
    "            p2 = p2.between_time(f'{ts2_1:02d}:00', f'{ts2_2:02d}:00')\n",
    "            p2['returns'] = p2['close'] - p2['open']\n",
    "\n",
    "            if len(p1['returns']) > 0 and len(p2['returns']) > 0:\n",
    "                pearson_corr = np.corrcoef(p1['returns'].tail(bars), p2['returns'].tail(bars))[0, 1]\n",
    "\n",
    "                if abs(pearson_corr) > 0.5:\n",
    "                    results.append((ts1_1, ts1_2, ts2_1, ts2_2, pearson_corr))\n",
    "                    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a2f16a4f-79be-48f3-b2c4-04caaa8e3778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 14, 13, 14, -0.5069723012444712),\n",
       " (14, 15, 14, 15, -0.7897661689149376),\n",
       " (15, 16, 15, 16, -0.7678376747086922),\n",
       " (16, 17, 16, 17, -0.582745257763089),\n",
       " (17, 18, 17, 18, -0.5154347958641352),\n",
       " (19, 20, 19, 20, -0.556167066543711),\n",
       " (20, 21, 20, 21, -0.612978471752775),\n",
       " (21, 22, 21, 22, -0.5632127428717656)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_corr('AUDUSD.a', 'USDJPY.a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "525a6ec6-d45d-4601-8e0e-3d68d446a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v3_mini_corr(s1, s2, bars=100):\n",
    "    for offset in range(24):\n",
    "        for hour in range(24):\n",
    "            ts1_1 = hour\n",
    "            ts1_2 = (ts1_1 + 1) % 24\n",
    "            \n",
    "            ts2_1 = (hour + 1 + offset) % 24\n",
    "            ts2_2 = (ts2_1 + 1) % 24\n",
    "            \n",
    "            # Convert to string format for the 'between_time' function\n",
    "            ts1_1, ts1_2, ts2_1, ts2_2 = [f'{t:02d}:00' for t in [ts1_1, ts1_2, ts2_1, ts2_2]]\n",
    "            \n",
    "            p1 = get_rates(s1, mt5.TIMEFRAME_H1, 50000)\n",
    "            p1 = p1.between_time(ts1_1, ts1_2)\n",
    "            p1['returns'] = p1['close'] - p1['open']\n",
    "            \n",
    "            p2 = get_rates(s2, mt5.TIMEFRAME_H1, 50000)\n",
    "            p2 = p2.between_time(ts2_1, ts2_2)\n",
    "            p2['returns'] = p2['close'] - p2['open']\n",
    "            \n",
    "            if len(p1['returns']) < bars or len(p2['returns']) < bars:\n",
    "                print(f'Insufficient data for {s1} between {ts1_1} and {ts1_2} or {s2} between {ts2_1} and {ts2_2}')\n",
    "                continue\n",
    "            \n",
    "            pearson_corr = np.corrcoef(p1['returns'].tail(bars), p2['returns'].tail(bars))[0, 1]\n",
    "            \n",
    "            if abs(pearson_corr) > 0.5:\n",
    "                print(f'Found correlation {pearson_corr} for {s1} between {ts1_1} and {ts1_2} and {s2} between {ts2_1} and {ts2_2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6d108713-eb0c-4df5-9011-234f986cf76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found correlation -0.7307982835086003 for AUDUSD.a between 14:00 and 15:00 and USDJPY.a between 14:00 and 15:00\n",
      "Found correlation -0.7374479429221158 for AUDUSD.a between 15:00 and 16:00 and USDJPY.a between 15:00 and 16:00\n",
      "Found correlation -0.5174647667371404 for AUDUSD.a between 16:00 and 17:00 and USDJPY.a between 16:00 and 17:00\n"
     ]
    }
   ],
   "source": [
    "v3_mini_corr('AUDUSD.a', 'USDJPY.a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a56a1042-8e99-454a-a2f0-50edf7efdb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lags that have a correlation higher than 0.5 are:\n",
      "[6, 7, 9, 10, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 33, 34, 35, 36, 39, 49]\n",
      "Total Lags: 20\n"
     ]
    }
   ],
   "source": [
    "#Correlation for AUDJPY Syd Open - USDJPY Tokyo Open\n",
    "lst = []\n",
    "\n",
    "un_corr = []\n",
    "\n",
    "for i in range(50):\n",
    "    pearson_corr = np.corrcoef(syd_opens['Syd Pip Return'].tail(i), tokyo_opens['Tko Pip Return'].tail(i))\n",
    "    if pearson_corr[0][1] > 0.5:\n",
    "        # print(f'Pearson correlation {pearson_corr[0][1]} on lag {i}')\n",
    "        lst.append(i)\n",
    "    else:\n",
    "        un_corr.append(i)\n",
    "        \n",
    "print(f'Lags that have a correlation higher than 0.5 are:'\n",
    "      f'\\n{lst}')\n",
    "print(f'Total Lags: {len(lst)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cd56253-bbcd-49f0-92d4-cf31122b2bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days that have correlations between AUDUSD and USDJPY of more than 0.5 are:\n",
      "Tuesday: 4\n",
      "Wednesday: 3\n",
      "Friday: 6\n",
      "Monday: 5\n",
      "Thursday: 2\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "day_counts = {}\n",
    "\n",
    "for i in lst:\n",
    "    date = syd_opens.index[i]\n",
    "    day_name = date.day_name()\n",
    "\n",
    "    if day_name not in day_counts:\n",
    "        day_counts[day_name] = 0\n",
    "\n",
    "    day_counts[day_name] += 1\n",
    "print(f'Days that have correlations between AUDUSD and USDJPY of more than 0.5 are:')\n",
    "\n",
    "for day, count in day_counts.items():\n",
    "    print(f'{day}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f97e9f83-5b2a-4253-8bc9-21f841a769c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days that do not have correlations between AUDUSD and USDJPY of more than 0.5 are:\n",
      "Monday: 5\n",
      "Tuesday: 6\n",
      "Wednesday: 7\n",
      "Thursday: 8\n",
      "Friday: 4\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "day_counts = {}\n",
    "\n",
    "for i in un_corr:\n",
    "    date = syd_opens.index[i]\n",
    "    day_name = date.day_name()\n",
    "\n",
    "    if day_name not in day_counts:\n",
    "        day_counts[day_name] = 0\n",
    "\n",
    "    day_counts[day_name] += 1\n",
    "print(f'Days that do not have correlations between AUDUSD and USDJPY of more than 0.5 are:')\n",
    "\n",
    "for day, count in day_counts.items():\n",
    "    print(f'{day}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55160da7-5dbc-4204-a3d2-7ff88f27d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_tool(symbol1, symbol2, timeframe, bars):\n",
    "    print(f'Starting Pearson Correlation Analysis for {symbol1}, {symbol2} at {timeframe} for {bars} bars.')\n",
    "\n",
    "    s1 = input(f'Start Time for {symbol1}?')\n",
    "    s2 = input(f'End time for {symbol1}?')\n",
    "\n",
    "    e1 = input(f'Start time for {symbol2}?')\n",
    "    e2 = input(f'End time for {symbol2}')\n",
    "\n",
    "    pair1 = get_rates(symbol1, timeframe, bars)\n",
    "    explored_seg1 = pair1.between_time(s1, s2)\n",
    "    pair2 = get_rates(symbol2,  timeframe, bars)\n",
    "    explored_seg2 = pair2.between_time(e1, e2)\n",
    "    \n",
    "    print('Times to investigate are:'\n",
    "          f'\\n{s1} to {s2} and {e1} to {e2}')\n",
    "    \n",
    "    pair1_returns = pair1['open'] - pair1['close']\n",
    "    pair2_returns = pair2['open'] - pair2['close']\n",
    "\n",
    "    lst = []\n",
    "\n",
    "    un_corr = []\n",
    "\n",
    "    for i in range(50):\n",
    "        pearson_corr = np.corrcoef(pair1_returns.tail(50), pair2_returns.tail(50))\n",
    "        if pearson_corr[0][1] > 0.5:\n",
    "            # print(f'Pearson correlation {pearson_corr[0][1]} on lag {i}')\n",
    "            lst.append(i)\n",
    "        else:\n",
    "            un_corr.append(i)\n",
    "            \n",
    "    print(f'Lags that have a correlation higher than 0.5 are:'\n",
    "        f'\\n{lst}')\n",
    "    print(f'Total Lags: {len(lst)}')\n",
    "\n",
    "    import datetime\n",
    "\n",
    "    day_counts = {}\n",
    "\n",
    "    for i in lst:\n",
    "        date = syd_opens.index[i]\n",
    "        day_name = date.day_name()\n",
    "\n",
    "        if day_name not in day_counts:\n",
    "            day_counts[day_name] = 0\n",
    "\n",
    "        day_counts[day_name] += 1\n",
    "    print(f'Days that have correlations between {symbol1} and {symbol2}of more than 0.5 are:')\n",
    "\n",
    "    for day, count in day_counts.items():\n",
    "        print(f'{day}: {count}')\n",
    "        \n",
    "    day_counts = {}\n",
    "\n",
    "    for i in un_corr:\n",
    "        date = syd_opens.index[i]\n",
    "        day_name = date.day_name()\n",
    "\n",
    "        if day_name not in day_counts:\n",
    "            day_counts[day_name] = 0\n",
    "\n",
    "        day_counts[day_name] += 1\n",
    "    print(f'Days that do not have correlations between {symbol1} and {symbol2} of more than 0.5 are:')\n",
    "\n",
    "    for day, count in day_counts.items():\n",
    "        print(f'{day}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee2f2447-cb6e-4a6b-a5cf-8411c9ad877a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pearson Correlation Analysis for EURUSD.a, GBPUSD.a at 16385 for 50000 bars.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Start Time for EURUSD.a? 10:00\n",
      "End time for EURUSD.a? 10:59\n",
      "Start time for GBPUSD.a? 16:00\n",
      "End time for GBPUSD.a 16:59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times to investigate are:\n",
      "10:00 to 10:59 and 16:00 to 16:59\n",
      "Lags that have a correlation higher than 0.5 are:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "Total Lags: 50\n",
      "Days that have correlations between EURUSD.a and GBPUSD.aof more than 0.5 are:\n",
      "Monday: 10\n",
      "Tuesday: 10\n",
      "Wednesday: 10\n",
      "Thursday: 10\n",
      "Friday: 10\n",
      "Days that do not have correlations between EURUSD.a and GBPUSD.a of more than 0.5 are:\n"
     ]
    }
   ],
   "source": [
    "corr_tool('EURUSD.a', 'GBPUSD.a', mt5.TIMEFRAME_H1, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a304af5-e108-48b7-8f05-e087a0dc554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair1 = get_rates('EURUSD.a', mt5.TIMEFRAME_H1, 10000)\n",
    "pair2 = get_rates('GBPUSD.a',  mt5.TIMEFRAME_H1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f06e54-75ed-4647-b0ac-1301562cea14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30299294-8d8e-4afa-867c-9e4140bce261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
