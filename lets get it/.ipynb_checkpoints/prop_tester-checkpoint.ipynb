{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b36937-8a83-4dc9-a294-9566275ccb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MetaTrader5 as mt5 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller \n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def get_rates(pair1, timeframe, x):\n",
    "    pair1 = pd.DataFrame(mt5.copy_rates_from_pos(pair1, timeframe, 0, x))\n",
    "    pair1['time'] = pd.to_datetime(pair1['time'], unit = 's')\n",
    "    return pair1[['time','open', 'high', 'low', 'close']].set_index('time')\n",
    "def calc_stats(df):\n",
    "\n",
    "    stats_df = pd.DataFrame()\n",
    "    stats_df['mean'] = df.mean()\n",
    "    stats_df['std'] = df.std()\n",
    "    # print('calculated')\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f116d-9b2f-4eea-aa50-4393f0c0fc2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b0dd9-0c87-44aa-803d-07674f89a8bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "id": "43560fbf-95bf-48d5-9a20-e59499338ed8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting AUDUSD.a's rates\n",
      "Getting USDCAD.a's rates\n",
      "Getting USDCHF.a's rates\n",
      "Getting GBPUSD.a's rates\n",
      "Getting USDJPY.a's rates\n",
      "Getting NZDUSD.a's rates\n",
      "Getting EURUSD.a's rates\n"
     ]
    }
   ],
   "source": [
    "combined_dfs_m = {}\n",
    "for country in count_codes:\n",
    "\n",
    "    # Define the endpoint parameters\n",
    "    flow = 'BIS,WS_EER_M,1.0'  # Example: Version 1.0 of the WS_EER_M domain, maintained by the BIS\n",
    "    key = f'M.N.N.{country[1]}'\n",
    "    start_period = '2000'  # Example: Start year 2000\n",
    "    end_period = '2024'  # Example: End year 2020\n",
    "    detail = 'full'  # Example: All data and documentation\n",
    "\n",
    "    # Construct the endpoint URL\n",
    "    endpoint_url = f'{base_url}/data/{flow}/{key}/all'\n",
    "\n",
    "    # Define the query parameters\n",
    "    query_params = {\n",
    "        'startPeriod': start_period,\n",
    "        'endPeriod': end_period,\n",
    "        'detail': detail\n",
    "    }\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(endpoint_url, params=query_params)\n",
    "\n",
    "    # Check for a successful response\n",
    "    if response.status_code == 200:\n",
    "        # Assign the text of the response to xml_data\n",
    "        xml_data = response.text\n",
    "\n",
    "        # Parse the XML data\n",
    "        root = ET.fromstring(xml_data)\n",
    "\n",
    "        # Initialize empty lists to store the data\n",
    "        time_periods = []\n",
    "        obs_values = []\n",
    "\n",
    "        # Iterate through the XML and extract the desired information\n",
    "        for obs in root.findall(\".//Obs\"):\n",
    "            time_period = obs.get('TIME_PERIOD')\n",
    "            obs_value = obs.get('OBS_VALUE')\n",
    "            time_periods.append(time_period)\n",
    "            obs_values.append(obs_value)\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Time_Period': time_periods,\n",
    "            'OBS_Value': obs_values\n",
    "        })\n",
    "\n",
    "        df['OBS_Value'] = df['OBS_Value'].replace('NaN', np.nan)\n",
    "        # Drop rows with NaN values\n",
    "        df.dropna(subset=['OBS_Value'], inplace=True)\n",
    "        df['OBS_Value'] = df['OBS_Value'].astype(float)\n",
    "        df['Time_Period'] = pd.to_datetime(df['Time_Period'])\n",
    "        \n",
    "        df = df.set_index('Time_Period')\n",
    "        print(f\"Getting {country[0]}'s rates\")\n",
    "        rates = get_rates(country[0], mt5.TIMEFRAME_D1, 2500)\n",
    "        \n",
    "        combined = pd.concat([df[-len(rates):], rates['close']], join = 'outer', axis = 1)\n",
    "        \n",
    "        combined_dfs_m[country[1]] = combined.dropna()\n",
    "\n",
    "    else:\n",
    "        print(f'Failed to retrieve data: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "id": "7a5abb7b-8c0e-4b61-acdf-1c7eaf141cde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting AUDUSD.a's rates\n",
      "Getting USDCAD.a's rates\n",
      "Getting USDCHF.a's rates\n",
      "Getting GBPUSD.a's rates\n",
      "Getting USDJPY.a's rates\n",
      "Getting NZDUSD.a's rates\n",
      "Getting EURUSD.a's rates\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "combined_dfs = {}\n",
    "\n",
    "# Define the base URL for the BIS Stats API\n",
    "base_url = 'https://stats.bis.org/api/v1'\n",
    "\n",
    "count_codes = [['AUDUSD.a','AU'], ['USDCAD.a', 'CA'],\n",
    "               ['USDCHF.a','CH'], ['GBPUSD.a', 'GB'],\n",
    "               ['USDJPY.a', 'JP'], ['NZDUSD.a', 'NZ'], \n",
    "               ['EURUSD.a', 'XM']]\n",
    "\n",
    "for country in count_codes:\n",
    "\n",
    "    # Define the endpoint parameters\n",
    "    flow = 'BIS,WS_EER_D,1.0'  # Example: Version 1.0 of the WS_EER_M domain, maintained by the BIS\n",
    "    key = f'D.N.N.{country[1]}'\n",
    "    start_period = '2000'  # Example: Start year 2000\n",
    "    end_period = '2025'  # Example: End year 2020\n",
    "    detail = 'full'  # Example: All data and documentation\n",
    "\n",
    "    # Construct the endpoint URL\n",
    "    endpoint_url = f'{base_url}/data/{flow}/{key}/all'\n",
    "\n",
    "    # Define the query parameters\n",
    "    query_params = {\n",
    "        'startPeriod': start_period,\n",
    "        'endPeriod': end_period,\n",
    "        'detail': detail\n",
    "    }\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(endpoint_url, params=query_params)\n",
    "\n",
    "    # Check for a successful response\n",
    "    if response.status_code == 200:\n",
    "        # Assign the text of the response to xml_data\n",
    "        xml_data = response.text\n",
    "\n",
    "        # Parse the XML data\n",
    "        root = ET.fromstring(xml_data)\n",
    "\n",
    "        # Initialize empty lists to store the data\n",
    "        time_periods = []\n",
    "        obs_values = []\n",
    "\n",
    "        # Iterate through the XML and extract the desired information\n",
    "        for obs in root.findall(\".//Obs\"):\n",
    "            time_period = obs.get('TIME_PERIOD')\n",
    "            obs_value = obs.get('OBS_VALUE')\n",
    "            time_periods.append(time_period)\n",
    "            obs_values.append(obs_value)\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Time_Period': time_periods,\n",
    "            'OBS_Value': obs_values\n",
    "        })\n",
    "\n",
    "        df['OBS_Value'] = df['OBS_Value'].replace('NaN', np.nan)\n",
    "        # Drop rows with NaN values\n",
    "        df.dropna(subset=['OBS_Value'], inplace=True)\n",
    "        df['OBS_Value'] = df['OBS_Value'].astype(float)\n",
    "        df['Time_Period'] = pd.to_datetime(df['Time_Period'])\n",
    "        \n",
    "        df = df.set_index('Time_Period')\n",
    "        print(f\"Getting {country[0]}'s rates\")\n",
    "        rates = get_rates(country[0], mt5.TIMEFRAME_D1, 2500)\n",
    "        \n",
    "        combined = pd.concat([df[-len(rates):], rates['close']], join = 'outer', axis = 1)\n",
    "        \n",
    "        combined_dfs[country[1]] = combined.dropna()\n",
    "\n",
    "    else:\n",
    "        print(f'Failed to retrieve data: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "edf6fa58-e78f-42e3-88f8-113698843fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Expected Returns\n",
    "weekly_dfs = {}\n",
    "features = ['OBS_Value', 'close']\n",
    "\n",
    "for name, dfs in combined_dfs.items():\n",
    "    df = combined_dfs[name]\n",
    "    # Assuming 'df' is your DataFrame\n",
    "    df['date'] = pd.to_datetime(df.index)\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    # Resample to get the last value of each week\n",
    "    weekly = df.resample('W').last()\n",
    "    weekly_dfs[name] = weekly\n",
    "    \n",
    "    # Add a new column for Monthly OBS Values in the weekly DataFrame\n",
    "    weekly_dfs[name]['Monthly_OBS_Value'] = np.nan\n",
    "\n",
    "    # Iterate through the monthly DataFrame\n",
    "    for date, row in combined_dfs_m[name].iterrows():\n",
    "        # Find all weekly dates that are in the same month and year as the monthly date\n",
    "        mask = (weekly_dfs[name].index.month == date.month) & (weekly_dfs[name].index.year == date.year)\n",
    "        weekly_dfs[name].loc[mask, 'Monthly_OBS_Value'] = row['OBS_Value']\n",
    "\n",
    "    # Forward fill the NaN values in the Monthly_OBS_Value column\n",
    "    weekly_dfs[name]['Monthly_OBS_Value'] = weekly_dfs[name]['Monthly_OBS_Value'].ffill()\n",
    "    \n",
    "for df in weekly_dfs.values():\n",
    "    df['W_EER_ret'] = df['OBS_Value'].pct_change()\n",
    "    df['M_EER_ret'] = df['Monthly_OBS_Value'].pct_change()\n",
    "    df['close_ret'] = df['close'].pct_change()\n",
    "    df['ret_diff'] = df['close_ret'] - df['W_EER_ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "id": "12e5ede1-4fb1-49d4-8d7f-27598e34719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_update = {}\n",
    "\n",
    "for name, df in weekly_dfs.items():\n",
    "    last_update[name] = df['W_EER_ret'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "id": "0869aadf-9a7c-4285-b536-f344a4c6787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_update.keys()\n",
    "symbols = ['AUDUSD.a', 'USDCAD.a', 'USDCHF.a', 'GBPUSD.a', 'USDJPY.a', 'NZDUSD.a', 'EURUSD.a']\n",
    "\n",
    "last_update = {symbols[i]: value for i, (key, value) in enumerate(last_update.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc1973-ac1d-4854-ad0f-dabe2f60359c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "907b78d8-f81b-429b-8204-b9c14ea9f6ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "id": "48701024-0e25-4997-ba64-78f0a60f1592",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --------- Testing variable list ['W_EER_ret', 'M_EER_ret'] --------- \n",
      "Looping through AU\n",
      "Coefficient of determination (R^2): 0.519629\n",
      "**Model with ['W_EER_ret', 'M_EER_ret'] performs better. Updating best model.**\n",
      " --------- Testing variable list ['W_EER_ret'] --------- \n",
      "Looping through AU\n",
      "Coefficient of determination (R^2): 0.734967\n",
      "**Model with ['W_EER_ret'] performs better. Updating best model.**\n",
      " --------- Testing variable list ['W_EER_ret', 'M_EER_ret'] --------- \n",
      "Looping through CA\n",
      "Coefficient of determination (R^2): 0.581836\n",
      "**Model with ['W_EER_ret', 'M_EER_ret'] performs better. Updating best model.**\n",
      " --------- Testing variable list ['W_EER_ret'] --------- \n",
      "Looping through CA\n",
      "Coefficient of determination (R^2): 0.717895\n",
      "**Model with ['W_EER_ret'] performs better. Updating best model.**\n",
      " --------- Testing variable list ['W_EER_ret', 'M_EER_ret'] --------- \n",
      "Looping through CH\n",
      "Coefficient of determination (R^2): 0.214635\n",
      "**Model with ['W_EER_ret', 'M_EER_ret'] performs better. Updating best model.**\n",
      " --------- Testing variable list ['W_EER_ret'] --------- \n",
      "Looping through CH\n",
      "Coefficient of determination (R^2): 0.456519\n",
      "**Model with ['W_EER_ret'] performs better. Updating best model.**\n",
      " --------- Testing variable list ['W_EER_ret', 'M_EER_ret'] --------- \n",
      "Looping through GB\n",
      "Coefficient of determination (R^2): 0.409005\n",
      "**Model with ['W_EER_ret', 'M_EER_ret'] performs better. Updating best model.**\n",
      " --------- Testing variable list ['W_EER_ret'] --------- \n",
      "Looping through GB\n",
      "Coefficient of determination (R^2): 0.475324\n",
      "**Model with ['W_EER_ret'] performs better. Updating best model.**\n",
      " --------- Testing variable list ['W_EER_ret', 'M_EER_ret'] --------- \n",
      "Looping through JP\n",
      "Coefficient of determination (R^2): 0.722574\n",
      "**Model with ['W_EER_ret', 'M_EER_ret'] performs better. Updating best model.**\n",
      " --------- Testing variable list ['W_EER_ret'] --------- \n",
      "Looping through JP\n",
      "Coefficient of determination (R^2): 0.595540\n",
      " --------- Testing variable list ['W_EER_ret', 'M_EER_ret'] --------- \n",
      "Looping through NZ\n",
      "Coefficient of determination (R^2): 0.483506\n",
      "**Model with ['W_EER_ret', 'M_EER_ret'] performs better. Updating best model.**\n",
      " --------- Testing variable list ['W_EER_ret'] --------- \n",
      "Looping through NZ\n",
      "Coefficient of determination (R^2): 0.703964\n",
      "**Model with ['W_EER_ret'] performs better. Updating best model.**\n",
      " --------- Testing variable list ['W_EER_ret', 'M_EER_ret'] --------- \n",
      "Looping through XM\n",
      "Coefficient of determination (R^2): 0.492443\n",
      "**Model with ['W_EER_ret', 'M_EER_ret'] performs better. Updating best model.**\n",
      " --------- Testing variable list ['W_EER_ret'] --------- \n",
      "Looping through XM\n",
      "Coefficient of determination (R^2): 0.613865\n",
      "**Model with ['W_EER_ret'] performs better. Updating best model.**\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "models = {}\n",
    "best_model_vars_dict = {}\n",
    "\n",
    "X_vars = [['W_EER_ret', 'M_EER_ret'], ['W_EER_ret']]\n",
    "for df in weekly_dfs:\n",
    "    best_score = float('-inf')  # Initialize the best score as negative infinity\n",
    "    best_model = None  # Initialize the best model\n",
    "    for variables in X_vars:\n",
    "        print(f\" --------- Testing variable list {variables} --------- \")\n",
    "        print(f\"Looping through {df}\")\n",
    "        X = weekly_dfs[df][variables].dropna()\n",
    "        y = weekly_dfs[df]['close_ret'].dropna()\n",
    "        y = y.iloc[-len(X):]\n",
    "\n",
    "        # Splitting the dataset into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "        # Creating the linear regression model\n",
    "        model = LinearRegression()\n",
    "        \n",
    "        # Training the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Making predictions using the testing set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # The coefficients\n",
    "        # print('Coefficients:', model.coef_)\n",
    "        # The mean squared error\n",
    "        # print('Mean squared error (MSE): %.6f' % mean_squared_error(y_test, y_pred))\n",
    "        # The coefficient of determination: 1 is perfect prediction\n",
    "        print('Coefficient of determination (R^2): %.6f' % r2_score(y_test, y_pred))\n",
    "        new_score = r2_score(y_test, y_pred)\n",
    "        # Check if this model is better\n",
    "        if new_score > best_score:\n",
    "            print(f\"**Model with {variables} performs better. Updating best model.**\")\n",
    "            best_score = new_score\n",
    "            best_model = model\n",
    "            best_model_vars = variables\n",
    "            \n",
    "    models[df] = best_model\n",
    "    best_model_vars_dict[df] = best_model_vars  # Store the best variables in the dictionary\n",
    "\n",
    "    stats[df] = calc_stats(weekly_dfs[df][best_model_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "id": "4ce8dfb4-ddaa-4207-a602-93a2ac0f87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_returns = pd.DataFrame()\n",
    "for sym in count_codes:\n",
    "    # print(f'Iterating thru {sym}')\n",
    "    rates = get_rates(sym[0], mt5.TIMEFRAME_D1, 2000)\n",
    "    rates = rates['close']\n",
    "    rates = rates.rename(f'{sym[0]}_close')\n",
    "    all_returns = pd.concat([all_returns, rates], axis = 1)\n",
    "latest_prices = all_returns.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b1337-6997-4bad-8f39-a6fffaaa8d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "135b4fb4-8547-4acc-9a30-7670b53adf23",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MC Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41d7edca-3bf2-4034-8a00-40604cd7c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pypfopt import EfficientFrontier, risk_models, expected_returns\n",
    "from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "db7dcf59-7a1d-4457-b261-027749b20860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_weights(weights):\n",
    "\n",
    "    total_weights = __builtins__.sum(weights.values)\n",
    "    \n",
    "    scaled = {currency: weight / total_weights * 100 for currency, weight in weights.items()}\n",
    "\n",
    "    if isinstance(weights, pd.Series):\n",
    "        return pd.Series(scaled)\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b1634b1a-5e19-4342-bde3-4f6f11b62bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_Sim(simulations, portfolio_val):\n",
    "\n",
    "    all_weights = []\n",
    "\n",
    "    for i in range(simulations):\n",
    "\n",
    "        if (i + 1) % 250 == 0:\n",
    "            print(f'{i + 1} simulations ran')\n",
    "\n",
    "        expected_return_lst = {}\n",
    "        for df, values in stats.items():\n",
    "            mean = values['mean']\n",
    "            std = values['std']\n",
    "\n",
    "            for df2, lr_model in models.items():\n",
    "                if df == df2:\n",
    "                    num_features = len(best_model_vars_dict[df2])  # Number of features the model was trained on\n",
    "                    # Ensure mean and std have correct dimensions\n",
    "                    mean_values = np.full(num_features, mean)\n",
    "                    std_values = np.full(num_features, std)\n",
    "                    # Generate random values for each feature\n",
    "                    random_eer_ret = np.random.normal(mean_values, std_values, (1, num_features))\n",
    "                    # Predict using the model\n",
    "                    expected_return_lst[df2] = lr_model.predict(random_eer_ret)\n",
    "\n",
    "        lst = []\n",
    "        for i in expected_return_lst.values():\n",
    "            lst.append(i)\n",
    "\n",
    "        highest_return = max(lst)\n",
    "        desired_scale = 0.05  # 5%\n",
    "        scaling_factor = desired_scale / highest_return\n",
    "\n",
    "        # Apply the scaling factor\n",
    "        scaled_returns = [ret * scaling_factor for ret in lst] \n",
    "        scaled_values = [x[0] * 100 for x in scaled_returns]\n",
    "        fin = []\n",
    "        for i in scaled_values:\n",
    "            fin.append(i) # changed from ABS # \n",
    "\n",
    "        # Calculate expected returns and the covariance matrix\n",
    "        mu = fin\n",
    "        S = risk_models.sample_cov(all_returns)\n",
    "\n",
    "        # Optimize for the minimised volatility\n",
    "        ef = EfficientFrontier(mu, S, weight_bounds = (-0.25, 0.25)) ## Changed weight bounds to reflect a portfolio with shorting\n",
    "        weights = ef.efficient_risk(0.025, market_neutral = False) ## Switched from True to False to reflect shorting\n",
    "        cleaned_weights = ef.clean_weights()\n",
    "\n",
    "        exp_ret, exp_vol, sharpe = ef.portfolio_performance()\n",
    "        da = DiscreteAllocation(\n",
    "        cleaned_weights, latest_prices, \n",
    "        total_portfolio_value = portfolio_val, short_ratio = None)\n",
    "\n",
    "        # Perform the discrete allocation\n",
    "        allocation, leftover = da.greedy_portfolio()\n",
    "        # Calculate the total investment in each asset\n",
    "        total_investment = {ticker: allocation[ticker] * latest_prices[ticker] for ticker in allocation}\n",
    "\n",
    "        # Total value of the portfolio\n",
    "        total_portfolio_value = sum(total_investment.values()) + leftover\n",
    "\n",
    "        # Convert to percentages\n",
    "        portfolio_percentages = {ticker: (value / total_portfolio_value) * 100 for ticker, value in total_investment.items()}\n",
    "\n",
    "        # Store the weights from each simulation\n",
    "        # all_weights[i, :] = np.array(list(portfolio_percentages.values()))\n",
    "        all_weights.append([portfolio_percentages, exp_ret, exp_vol])\n",
    "        \n",
    "    return all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "id": "88e8f825-e0a9-482e-b5cf-950ab4279b00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 simulations ran\n",
      "500 simulations ran\n",
      "750 simulations ran\n",
      "1000 simulations ran\n",
      "1250 simulations ran\n",
      "1500 simulations ran\n",
      "1750 simulations ran\n",
      "2000 simulations ran\n",
      "2250 simulations ran\n",
      "2500 simulations ran\n",
      "2750 simulations ran\n",
      "3000 simulations ran\n",
      "3250 simulations ran\n",
      "3500 simulations ran\n",
      "3750 simulations ran\n",
      "4000 simulations ran\n",
      "4250 simulations ran\n",
      "4500 simulations ran\n",
      "4750 simulations ran\n",
      "5000 simulations ran\n",
      "5250 simulations ran\n",
      "5500 simulations ran\n",
      "5750 simulations ran\n",
      "6000 simulations ran\n",
      "6250 simulations ran\n",
      "6500 simulations ran\n",
      "6750 simulations ran\n",
      "7000 simulations ran\n",
      "7250 simulations ran\n",
      "7500 simulations ran\n",
      "7750 simulations ran\n",
      "8000 simulations ran\n"
     ]
    }
   ],
   "source": [
    "all_weights = MC_Sim(8000, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "125d68c2-4c26-4a3d-8006-43da7e7597b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Portfolio Weights:\n",
      "AUDUSD.a_close     7.536998\n",
      "USDCAD.a_close    23.623394\n",
      "USDCHF.a_close    21.525681\n",
      "EURUSD.a_close    20.959173\n",
      "GBPUSD.a_close     6.498291\n",
      "NZDUSD.a_close     7.029920\n",
      "USDJPY.a_close    12.516694\n",
      "dtype: float64\n",
      "\n",
      "Mean Expected Return: 2.322274875279652\n",
      "Mean Expected Volatility: 0.025000001174502975\n"
     ]
    }
   ],
   "source": [
    "weights_list = []\n",
    "returns_list = []\n",
    "volatility_list = []\n",
    "\n",
    "# Extract values into separate lists\n",
    "for item in all_weights:\n",
    "    weights_list.append(item[0])  # Portfolio weights dictionary\n",
    "    returns_list.append(item[1])  # Expected return\n",
    "    volatility_list.append(item[2])  # Expected volatility\n",
    "\n",
    "# Convert weights to DataFrame for easier mean calculation\n",
    "df_weights = pd.DataFrame(weights_list)\n",
    "\n",
    "# Calculate mean weights, returns, and volatility\n",
    "mean_weights = df_weights.mean()\n",
    "mean_return = np.mean(returns_list)\n",
    "mean_volatility = np.mean(volatility_list)\n",
    "\n",
    "print(\"Mean Portfolio Weights:\")\n",
    "print(mean_weights)\n",
    "print(\"\\nMean Expected Return:\", mean_return)\n",
    "print(\"Mean Expected Volatility:\", mean_volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "id": "513879b2-189c-4df8-9d60-4156baa13474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights_list = []\n",
    "returns_list = []\n",
    "volatility_list = []\n",
    "\n",
    "# Extract values into separate lists\n",
    "for item in all_weights:\n",
    "    weights_list.append(item[0])  # Portfolio weights dictionary\n",
    "    returns_list.append(item[1])  # Expected return\n",
    "    volatility_list.append(item[2])  # Expected volatility\n",
    "\n",
    "# Convert weights to DataFrame for easier mean and IQR calculation\n",
    "df_weights = pd.DataFrame(weights_list)\n",
    "\n",
    "# Calculate mean weights and IQR for each asset\n",
    "mean_weights = df_weights.mean()\n",
    "iqr_weights = df_weights.quantile(0.75) - df_weights.quantile(0.25)\n",
    "iqr_25_weights = df_weights.quantile(0.25)\n",
    "iqr_75_weights = df_weights.quantile(0.75)\n",
    "\n",
    "# Combine return and volatility data\n",
    "df_performance = pd.DataFrame({\n",
    "    'expected_return': returns_list,\n",
    "    'expected_volatility': volatility_list\n",
    "})\n",
    "\n",
    "# Calculate mean return and volatility\n",
    "mean_return = df_performance['expected_return'].mean()\n",
    "iqr_25_ret = df_performance['expected_return'].quantile(0.25)\n",
    "iqr_75_ret = df_performance['expected_return'].quantile(0.75)\n",
    "\n",
    "mean_volatility = df_performance['expected_volatility'].mean()\n",
    "iqr_25_vol = df_performance['expected_volatility'].quantile(0.25)\n",
    "iqr_75_vol = df_performance['expected_volatility'].quantile(0.75)\n",
    "\n",
    "iqr_25_dict = {}\n",
    "iqr_75_dict = {}\n",
    "mean_dict = {}\n",
    "\n",
    "for index, row in df_weights.iterrows():\n",
    "    row_sum = row.sum()\n",
    "    shortfall = 100 - row_sum\n",
    "    null_columns = row[row.isnull()].index  # Identify columns with NaN values\n",
    "    df_weights.loc[index, null_columns] = shortfall / len(null_columns)  # Distribute the shortfall evenly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "id": "cb117cdd-029e-45ff-9552-19d7ada40dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_25_dict['weights'] = iqr_25_weights\n",
    "iqr_75_dict['weights'] = iqr_75_weights\n",
    "mean_dict['weights'] = mean_weights\n",
    "\n",
    "iqr_25_dict['ret'] = iqr_25_ret\n",
    "iqr_75_dict['ret'] = iqr_75_ret\n",
    "mean_dict['ret'] = mean_return\n",
    "\n",
    "iqr_25_dict['vol'] = iqr_25_vol\n",
    "iqr_75_dict['vol'] = iqr_75_vol\n",
    "mean_dict['vol'] = mean_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "id": "7e911636-4653-4f4b-b0a8-b5eea32dcaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through iqr25\n",
      "iqr25 has the highest return at 0.9967296576728408 with weights\n",
      "AUDUSD.a_close    -7.504688\n",
      "USDCAD.a_close    23.688593\n",
      "USDCHF.a_close    17.768128\n",
      "EURUSD.a_close    17.745703\n",
      "GBPUSD.a_close    -1.199485\n",
      "NZDUSD.a_close    -7.256452\n",
      "USDJPY.a_close     6.133766\n",
      "Name: 0.25, dtype: float64\n",
      "Iterating through iqr75\n",
      "iqr75 has the highest return at 2.7450053786788553 with weights\n",
      "AUDUSD.a_close    25.140372\n",
      "USDCAD.a_close    25.447142\n",
      "USDCHF.a_close    25.511766\n",
      "EURUSD.a_close    25.547300\n",
      "GBPUSD.a_close    15.465178\n",
      "NZDUSD.a_close    25.069856\n",
      "USDJPY.a_close    16.598245\n",
      "Name: 0.75, dtype: float64\n",
      "Iterating through mean_dict\n"
     ]
    }
   ],
   "source": [
    "weights = {}\n",
    "all_dicts = {}\n",
    "\n",
    "all_dicts['iqr25'] = iqr_25_dict\n",
    "all_dicts['iqr75'] = iqr_75_dict\n",
    "all_dicts['mean_dict'] = mean_dict\n",
    "highest_ret = 0\n",
    "\n",
    "for df, items in all_dicts.items():\n",
    "\n",
    "    print(f\"Iterating through {df}\")\n",
    "    if all_dicts[df]['ret'] > highest_ret:\n",
    "        highest_ret = all_dicts[df]['ret']\n",
    "        weights = all_dicts[df]['weights']\n",
    "        print(f\"{df} has the highest return at {highest_ret} with weights\"\n",
    "              f\"\\n{weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506312bb-fad0-4687-b819-6d1eca8b5a56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Lot Size Allocation and Order Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "325a0be1-1453-41a8-b7c9-ec7ad456e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lot_sizes(monetary_allocation, latest_prices_dict, lot_size_unit):\n",
    "    lot_sizes = {}\n",
    "    \n",
    "    latest_prices_dict = dict(latest_prices_dict)\n",
    "    \n",
    "    for currency_pair, allocation in monetary_allocation.items():\n",
    "        # Extract the latest price for the current currency pair\n",
    "        val = latest_prices_dict[currency_pair]\n",
    "\n",
    "        # Determine pip value based on the currency pair\n",
    "        if currency_pair[:3] == 'USD':\n",
    "            # USD is the base currency\n",
    "            if 'JPY' in currency_pair[3:]:  # JPY exception\n",
    "                pip_value = 10 / val * 100\n",
    "            else:\n",
    "                pip_value = 10 / val\n",
    "        else:\n",
    "            # USD is the quote currency or pair does not involve USD\n",
    "            pip_value = 10\n",
    "\n",
    "        # print(f\"Pip value for {currency_pair} is {pip_value}. Now calculating lots...\")\n",
    "\n",
    "        # Standard lot value calculation (100,000 units)\n",
    "        standard_lot_value = 100000 * pip_value\n",
    "\n",
    "        # Calculate lot size\n",
    "        lot_size = allocation / standard_lot_value\n",
    "\n",
    "        # Convert to desired lot size unit\n",
    "        if lot_size_unit == 'micro':\n",
    "            # 1 micro lot = 1/100th of a standard lot\n",
    "            lot_size *= 100\n",
    "        elif lot_size_unit == 'mini':\n",
    "            # 1 mini lot = 1/10th of a standard lot\n",
    "            lot_size *= 10\n",
    "\n",
    "        lot_sizes[currency_pair] = round(lot_size, 2)  # Rounding to 2 decimal places\n",
    "\n",
    "    return lot_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "id": "a1d447a1-bdff-4642-bccc-6ad249fa9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eer_os(last_update, lot_sizes, comment):\n",
    "    \n",
    "    for sym, lotsize in lot_sizes.items():\n",
    "\n",
    "        for symbol, value in last_update.items():\n",
    "\n",
    "            if sym[:8] == symbol:\n",
    "\n",
    "                if symbol[:3] == 'USD':\n",
    "                    print(f\"Looping through {symbol[:6]}\")\n",
    "                    if value > 0:\n",
    "                        send_order(symbol[:6], 'sell', lotsize, comment)\n",
    "                        print(f\"Selling {symbol[:6]}\")\n",
    "                    else:\n",
    "                        pass\n",
    "                        send_order(symbol[:6], 'buy', lotsize, comment)\n",
    "                else:\n",
    "                    print(f\"Looping through {symbol[:6]}\")\n",
    "                    if value > 0:\n",
    "                        print(f\"Buying {symbol[:6]}\")\n",
    "                        send_order(symbol[:6], 'buy', lotsize, comment)\n",
    "                    else:\n",
    "                        print(f\"Selling {symbol[:6]}\")\n",
    "                        send_order(symbol[:6], 'sell', lotsize, comment)\n",
    "                        \n",
    "def close_position(position, comment):\n",
    "    \n",
    "    tick = mt5.symbol_info_tick(position.symbol)\n",
    "\n",
    "    request = {\n",
    "        \"action\" : mt5.TRADE_ACTION_DEAL,\n",
    "        \"position\": position.ticket,\n",
    "        \"symbol\": position.symbol,\n",
    "        \"volume\": position.volume,\n",
    "        \"type\": mt5.ORDER_TYPE_BUY if position.type == 1 else mt5.ORDER_TYPE_SELL,\n",
    "        \"price\": tick.ask if position.type == 1 else tick.bid,\n",
    "        \"deviation\": 20,\n",
    "        \"magic\": 100,\n",
    "        \"comment\": comment,\n",
    "        'type_time': mt5.ORDER_TIME_GTC,\n",
    "        'type_filling':mt5.ORDER_FILLING_IOC,\n",
    "\n",
    "        }\n",
    "    result = mt5.order_send(request)\n",
    "    \n",
    "def send_order(symbol, side, lot, comment):\n",
    "    \n",
    "    if side.lower() == 'sell':\n",
    "        order_type = mt5.ORDER_TYPE_SELL\n",
    "        price = mt5.symbol_info_tick(symbol).bid\n",
    "    elif side.lower() == 'buy':\n",
    "        order_type = mt5.ORDER_TYPE_BUY\n",
    "        price = mt5.symbol_info_tick(symbol).ask\n",
    "    \n",
    "    request = {\n",
    "        \"action\": mt5.TRADE_ACTION_DEAL,\n",
    "        \"symbol\": symbol,\n",
    "        \"volume\": lot,\n",
    "        \"type\": order_type,\n",
    "        \"price\": price,\n",
    "        \"deviation\": 5,\n",
    "        \"magic\": 234000,\n",
    "        \"comment\": comment,\n",
    "        \"type_time\": mt5.ORDER_TIME_GTC,\n",
    "        \"type_filling\": mt5.ORDER_FILLING_IOC,\n",
    "    }\n",
    "    result = mt5.order_send(request)\n",
    "\n",
    "def close_all():\n",
    "    close_positions = []\n",
    "    open_positions = mt5.positions_get()\n",
    "    open_positions\n",
    "    for i in open_positions:\n",
    "        close_positions.append(i)\n",
    "        \n",
    "    for pos in close_positions:\n",
    "        close_position(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51974c90-d966-4e7f-8166-5e2cccab3a08",
   "metadata": {
    "tags": []
   },
   "source": [
    "# COT report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449fb5d1-f7df-4a54-bda0-ee820df65f47",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data / Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "087118dc-b385-4f4b-b485-3f02115c285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import cot_reports as cot\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "def get_rates(pair1, timeframe, x):\n",
    "    pair1 = pd.DataFrame(mt5.copy_rates_from_pos(pair1, timeframe, 0, x))\n",
    "    pair1['time'] = pd.to_datetime(pair1['time'], unit = 's')\n",
    "    return pair1[['time','open', 'high', 'low', 'close']].set_index('time')\n",
    "def get_rates_w(pair1, timeframe, x):\n",
    "    \n",
    "    pair1 = pd.DataFrame(mt5.copy_rates_from_pos(pair1, timeframe, 0, x))\n",
    "    \n",
    "    pair1['time'] = pd.to_datetime(pair1['time'], unit='s')\n",
    "    pair1 = pair1[['time', 'open', 'high', 'low', 'close']].set_index('time')\n",
    "    \n",
    "    # Resample to weekly data starting from the day after COT report release\n",
    "    weekly_data = pair1.resample('W-TUE', label='left').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    })\n",
    "\n",
    "    return weekly_data\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "def rf_training():\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    # Define the scoring function\n",
    "    mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "    # Instantiate the grid search model\n",
    "    grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, \n",
    "                                  cv=3, n_jobs=-1, scoring=mse_scorer, verbose=2)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameters\n",
    "    print(\"Best parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "\n",
    "    rf_best = grid_search_rf.best_params_\n",
    "    \n",
    "    return rf_best\n",
    "\n",
    "sym_lst = ['SWISS FRANC', 'BRITISH POUND STERLING', 'JAPANESE YEN', 'EURO FX', 'NEW ZEALAND DOLLAR', 'AUSTRALIAN DOLLAR', \n",
    "       'EURO FX/JAPANESE YEN XRATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "533f6b10-ffc8-4e02-8bf5-b0ecf1460dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: legacy_futopt\n",
      "Downloaded single year data from: 2015\n",
      "Selected: legacy_futopt\n",
      "Downloaded single year data from: 2016\n",
      "Selected: legacy_futopt\n",
      "Downloaded single year data from: 2017\n",
      "Selected: legacy_futopt\n",
      "Downloaded single year data from: 2018\n",
      "Selected: legacy_futopt\n",
      "Downloaded single year data from: 2019\n",
      "Selected: legacy_futopt\n",
      "Downloaded single year data from: 2020\n",
      "Selected: legacy_futopt\n",
      "Downloaded single year data from: 2021\n",
      "Selected: legacy_futopt\n",
      "Downloaded single year data from: 2022\n",
      "Selected: legacy_futopt\n",
      "Downloaded single year data from: 2023\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "begin_year = 2015\n",
    "end_year = 2023\n",
    "for i in range(begin_year, end_year + 1):\n",
    "    single_year = pd.DataFrame(cot.cot_year(i, cot_report_type='legacy_futopt')) \n",
    "    df = pd.concat([df, single_year], axis = 0)\n",
    "\n",
    "t1 = []\n",
    "t2 = []\n",
    "temp = df.set_index('Market and Exchange Names').T\n",
    "for col in temp.columns.unique():\n",
    "    if 'CHICAGO MERCANTILE EXCHANGE' in col:\n",
    "        t1.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "d5e38b7b-6241-488d-bed5-c3950d83ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = []\n",
    "t2 = []\n",
    "temp = df.set_index('Market and Exchange Names').T\n",
    "for col in temp.columns.unique():\n",
    "    if 'CHICAGO MERCANTILE EXCHANGE' in col:\n",
    "        t1.append(col)\n",
    "        \n",
    "for sym in sym_lst:\n",
    "    for instrument in t1:\n",
    "        if sym in instrument:\n",
    "            # print(f\"match found on {sym} and {instrument}\")\n",
    "            t2.append(instrument)\n",
    "selected_cols = temp[t2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "2a044b0d-4e24-4ced-8f79-a8ac999bf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_to_drop = [\n",
    "    \"Contract Units\",\n",
    "    \"CFTC Contract Market Code (Quotes)\",\n",
    "    \"CFTC Market Code in Initials (Quotes)\",\n",
    "    \"CFTC Commodity Code (Quotes)\",\n",
    "    \"CFTC Contract Market Code\",\n",
    "    \"CFTC Market Code in Initials\",\n",
    "    \"CFTC Region Code\",\n",
    "    \"CFTC Commodity Code\", \n",
    "    \n",
    "]\n",
    "\n",
    "df_dropped = selected_cols.T.drop(columns = fields_to_drop, errors = 'ignore')\n",
    "df_dict = dict(df_dropped.T)\n",
    "\n",
    "for name, df in df_dict.items():\n",
    "    # df[name]\n",
    "    df = df_dict[name]\n",
    "    df = df_dict[name].T.reset_index()\n",
    "    df = df.rename(columns = {'As of Date in Form YYYY-MM-DD' : 'Date'})\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace = True)\n",
    "    df = df.drop(columns = ['Market and Exchange Names', 'As of Date in Form YYMMDD'])\n",
    "    df_dict[name] = df\n",
    "    df_dict[name].sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "53e0a931-29fd-40c4-994a-81703c02dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_list = ['SWISS FRANC',\n",
    " 'BRITISH POUND STERLING',\n",
    " 'JAPANESE YEN',\n",
    " 'EURO FX/JAPANESE YEN XRATE',\n",
    " 'EURO FX',\n",
    " 'NEW ZEALAND DOLLAR',\n",
    " 'AUSTRALIAN DOLLAR']\n",
    "\n",
    "sym_rates = ['USDCHF.a', 'GBPUSD.a', 'USDJPY.a', 'EURJPY.a', 'EURUSD.a', 'NZDUSD.a', 'AUDUSD.a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "f9c9bcee-3113-4760-9f34-b443152cd6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cot_dict = dict(zip(cleaned_list, df_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "2c44edb6-f804-4715-8ad2-8dbcc5584bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info = dict(zip(cleaned_list, sym_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "9edc9742-70a9-4166-a65a-b9d4b232e196",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------- Iterating through USDCHF.a -------\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Mean squared error (MSE): 0.000526\n",
      "Coefficient of determination (R^2): 0.667727\n",
      " ------- Iterating through GBPUSD.a -------\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best parameters for Random Forest: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Mean squared error (MSE): 0.002557\n",
      "Coefficient of determination (R^2): 0.393156\n",
      " ------- Iterating through USDJPY.a -------\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best parameters for Random Forest: {'max_depth': 30, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Mean squared error (MSE): 57.592008\n",
      "Coefficient of determination (R^2): 0.720441\n",
      " ------- Iterating through EURJPY.a -------\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best parameters for Random Forest: {'max_depth': 30, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Mean squared error (MSE): 4.980027\n",
      "Coefficient of determination (R^2): 0.956943\n",
      " ------- Iterating through EURUSD.a -------\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best parameters for Random Forest: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Mean squared error (MSE): 0.000613\n",
      "Coefficient of determination (R^2): 0.787409\n",
      " ------- Iterating through NZDUSD.a -------\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Mean squared error (MSE): 0.000387\n",
      "Coefficient of determination (R^2): 0.812742\n",
      " ------- Iterating through AUDUSD.a -------\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best parameters for Random Forest: {'max_depth': 30, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Mean squared error (MSE): 0.000542\n",
      "Coefficient of determination (R^2): 0.720763\n"
     ]
    }
   ],
   "source": [
    "rf_results = {}\n",
    "for cot_data, sym in combined_info.items():\n",
    "    print(f\" ------- Iterating through {sym} -------\")\n",
    "    \n",
    "    cot_data = new_cot_dict[cot_data]\n",
    "    \n",
    "    # Convert all columns to numeric, replacing non-numeric values with NaN\n",
    "    for col in cot_data.columns:\n",
    "        cot_data[col] = pd.to_numeric(cot_data[col], errors='coerce')\n",
    "\n",
    "    # Iterate through columns and drop those with a sum of 0\n",
    "    cols_to_drop = [col for col in cot_data.columns if cot_data[col].sum() == 0]\n",
    "    cot_data.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    X = cot_data[['Open Interest (All)', 'Noncommercial Positions-Spreading (All)', 'Commercial Positions-Long (All)', \n",
    "                           'Commercial Positions-Short (All)', ' Total Reportable Positions-Long (All)', 'Total Reportable Positions-Short (All)',\n",
    "               'Traders-Noncommercial-Spreading (All)']]\n",
    "    X = X.dropna()\n",
    "    \n",
    "    rate = get_rates_w(sym, mt5.TIMEFRAME_D1, (len(X) * 5))\n",
    "    rate['close_lag1'] = rate['close'].shift(1)\n",
    "\n",
    "    y = rate['close_lag1']\n",
    "    length_diff = len(y) - len(X)\n",
    "    if length_diff != 0:\n",
    "        \n",
    "        y = y.iloc[length_diff:]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "        # Splitting the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "    rf_best = rf_training()\n",
    "    # Create the Random Forest model\n",
    "    model = RandomForestRegressor(**rf_best)  # You can tune n_estimators and other parameters\n",
    "    \n",
    "    # Training the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Making predictions using the testing set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # The coefficients\n",
    "    # print('Coefficients:', model.coef_)\n",
    "    # The mean squared error\n",
    "    print('Mean squared error (MSE): %.6f' % mean_squared_error(y_test, y_pred))\n",
    "    # The coefficient of determination: 1 is perfect prediction\n",
    "    print('Coefficient of determination (R^2): %.6f' % r2_score(y_test, y_pred))\n",
    "    \n",
    "    rf_results[sym] = [r2_score(y_test, y_pred), model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "d96dac64-34d0-44fc-a151-e3e206b96052",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USDCHF.a': [0.6704273127407699,\n",
       "  RandomForestRegressor(max_depth=10, n_estimators=300)],\n",
       " 'GBPUSD.a': [0.3699800637740899,\n",
       "  RandomForestRegressor(max_depth=20, n_estimators=200)],\n",
       " 'USDJPY.a': [0.7179412430517143,\n",
       "  RandomForestRegressor(max_depth=30, n_estimators=200)],\n",
       " 'EURJPY.a': [0.9586310372275183,\n",
       "  RandomForestRegressor(max_depth=30, n_estimators=300)],\n",
       " 'EURUSD.a': [0.7802222520268962,\n",
       "  RandomForestRegressor(max_depth=20, n_estimators=300)],\n",
       " 'NZDUSD.a': [0.811431227489253,\n",
       "  RandomForestRegressor(max_depth=10, n_estimators=200)],\n",
       " 'AUDUSD.a': [0.7134857444539408,\n",
       "  RandomForestRegressor(max_depth=30, n_estimators=200)]}"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimised_dict = {}\n",
    "for key in lin_results:\n",
    "    if key in rf_results:\n",
    "        # Compare RÂ² scores and choose the higher one\n",
    "        if lin_results[key][0] > rf_results[key][0]:\n",
    "            optimised_dict[key] = lin_results[key]\n",
    "        else:\n",
    "            optimised_dict[key] = rf_results[key]\n",
    "\n",
    "optimised_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ae725-4679-46cd-92a5-f56af9af3f8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "id": "4eff9c20-fc9b-48db-b6b3-407098896075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------- Iterating through USDCHF.a -------\n",
      "[0.88099279]\n",
      " ------- Iterating through GBPUSD.a -------\n",
      "[1.2746551]\n",
      " ------- Iterating through USDJPY.a -------\n",
      "[139.858915]\n",
      " ------- Iterating through EURJPY.a -------\n",
      "[157.35320829]\n",
      " ------- Iterating through EURUSD.a -------\n",
      "[1.09800097]\n",
      " ------- Iterating through NZDUSD.a -------\n",
      "[0.64023244]\n",
      " ------- Iterating through AUDUSD.a -------\n",
      "[0.69976125]\n"
     ]
    }
   ],
   "source": [
    "forecasts = {}\n",
    "for cot_data, sym in combined_info.items():\n",
    "    \n",
    "    print(f\" ------- Iterating through {sym} -------\")\n",
    "    \n",
    "    cot_data = new_cot_dict[cot_data]\n",
    "    \n",
    "    # Convert all columns to numeric, replacing non-numeric values with NaN\n",
    "    for col in cot_data.columns:\n",
    "        cot_data[col] = pd.to_numeric(cot_data[col], errors='coerce')\n",
    "\n",
    "    # Iterate through columns and drop those with a sum of 0\n",
    "    cols_to_drop = [col for col in cot_data.columns if cot_data[col].sum() == 0]\n",
    "    cot_data.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    features = cot_data[['Open Interest (All)', 'Noncommercial Positions-Spreading (All)', 'Commercial Positions-Long (All)', \n",
    "                           'Commercial Positions-Short (All)', ' Total Reportable Positions-Long (All)', 'Total Reportable Positions-Short (All)',\n",
    "                           'Traders-Noncommercial-Spreading (All)']]\n",
    "    \n",
    "    features = features.iloc[[-1]]  # Use double brackets to keep DataFrame structure\n",
    "    \n",
    "    for pair, model in optimised_dict.items():\n",
    "        if sym == pair:\n",
    "            prediction_model = model[1]\n",
    "            forecast = prediction_model.predict(features)  # features is now a DataFrame with correct shape\n",
    "            print(forecast)\n",
    "            forecasts[sym] = float(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "0a5a45e2-d907-4208-87e4-4706fc44f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_prices = {}\n",
    "\n",
    "for sym in sym_rates:\n",
    "    rate = get_rates(sym, mt5.TIMEFRAME_D1, 1)\n",
    "    current_prices[sym] = rate['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "6cc8e29b-c59c-4ea6-8ae0-3ca49238c498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8809927934351901 0.84936\n",
      "1.274655099999999 1.264\n",
      "139.85891500000008 141.935\n",
      "157.35320828571403 155.581\n",
      "1.0980009666666657 1.0961400000000001\n",
      "0.6402324383333337 0.62694\n",
      "0.6997612499999992 0.67652\n"
     ]
    }
   ],
   "source": [
    "cot_positions = {}\n",
    "\n",
    "for sym, forecast in forecasts.items():\n",
    "    for pair, cur_price in current_prices.items():\n",
    "        if sym == pair:\n",
    "            print(forecast, cur_price[0])\n",
    "            cot_positions[sym] = forecast - cur_price[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651149af-f7c8-4f24-ae8b-1ac1c78f40c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Funding Pips Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "id": "293f0d7b-f436-42e1-961a-f94b42ba0a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUDUSD.a': 0.004129850172877481,\n",
       " 'USDCAD.a': -0.0013612056392805494,\n",
       " 'USDCHF.a': -0.006891205099491704,\n",
       " 'GBPUSD.a': 0.0013651877133105117,\n",
       " 'USDJPY.a': 0.0005196154845412693,\n",
       " 'NZDUSD.a': -0.0022458744263254005,\n",
       " 'EURUSD.a': -0.0032070555221488384}"
      ]
     },
     "execution_count": 1039,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "id": "d777d666-cba9-4acf-ae75-c36d985ee7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.037037037037037035"
      ]
     },
     "execution_count": 1046,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.05 / 1.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c347bd1-06cd-4cd9-ace9-63dafd0b483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "mt5.initialize()\n",
    "# Replace following with your MT5 Account Login\n",
    "account = 926350\n",
    "password = 'ADByx_74hJ' \n",
    "server = 'BlackBullMarkets-FPDemo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c75d6-ad2b-4b3e-9aa9-923ce865b5d0",
   "metadata": {},
   "source": [
    "### Monte Carlo Strat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "id": "b7407c2e-1326-429c-88cf-a175ec28a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_value = 12500\n",
    "weights = scale_weights(weights)\n",
    "monetary_allocation = {pair: (weight / 100) * account_value for pair, weight in weights.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "id": "5b92e2af-8c84-48e9-8dfc-79061aa1698a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mc_lot_sizes = calculate_lot_sizes(monetary_allocation, latest_prices, 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "id": "0ae6f23d-6d6a-498d-8885-1f682d51cb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through AUDUSD\n",
      "Buying AUDUSD\n",
      "Looping through USDCAD\n",
      "Looping through USDCHF\n",
      "Looping through EURUSD\n",
      "Selling EURUSD\n",
      "Looping through GBPUSD\n",
      "Buying GBPUSD\n",
      "Looping through NZDUSD\n",
      "Selling NZDUSD\n",
      "Looping through USDJPY\n",
      "Selling USDJPY\n"
     ]
    }
   ],
   "source": [
    "eer_os(last_update, mc_lot_sizes, 'MC_eer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ddb6b-4329-4b32-b7cd-93ab88d88cb2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Simple Strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "a3ef6ce6-18b1-4baa-adf7-8673ae9ebc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0 \n",
    "for val in lot_sizes.values():\n",
    "    total += val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "6c16b8de-65e0-4fd6-a1d6-c83e4c5b3976",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USDCAD.a_close': 0.020000000000000004,\n",
       " 'USDCHF.a_close': 0.020000000000000004,\n",
       " 'EURUSD.a_close': 0.020000000000000004,\n",
       " 'AUDUSD.a_close': 0.020000000000000004,\n",
       " 'GBPUSD.a_close': 0.020000000000000004,\n",
       " 'NZDUSD.a_close': 0.020000000000000004,\n",
       " 'USDJPY.a_close': 0.020000000000000004}"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_sizes = {}\n",
    "\n",
    "for sym in lot_sizes.keys():\n",
    "    sim_sizes[sym] = float(round(total/7, 2) - 0.03)\n",
    "sim_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "19383913-7f82-46c5-b9e3-0545edd8be38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through USDCAD\n",
      "Selling USDCAD\n",
      "Looping through USDCHF\n",
      "Looping through EURUSD\n",
      "Buying EURUSD\n",
      "Looping through AUDUSD\n",
      "Buying AUDUSD\n",
      "Looping through GBPUSD\n",
      "Buying GBPUSD\n",
      "Looping through NZDUSD\n",
      "Selling NZDUSD\n",
      "Looping through USDJPY\n",
      "Selling USDJPY\n"
     ]
    }
   ],
   "source": [
    "eer_os(last_update, sim_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f35cd-ecff-4991-9e6f-9329240f5618",
   "metadata": {},
   "source": [
    "### COT Strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "471fb3ba-0413-4ffd-b0fa-2f925ba09346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going long on USDCHF\n",
      "Going long on GBPUSD\n",
      "Going short on USDJPY.a\n",
      "Going long on EURJPY\n",
      "Going long on EURUSD\n",
      "Going long on NZDUSD\n",
      "Going long on AUDUSD\n"
     ]
    }
   ],
   "source": [
    "for pair, direction in cot_positions.items():\n",
    "    if direction > 0:\n",
    "        print(f\"Going long on {pair[:6]}\")\n",
    "        # send_order(pair, 'buy', 0.02, 'cot_rep')\n",
    "    else:\n",
    "        print(f\"Going short on {pair}\")\n",
    "        # send_order(pair[:6], 'sell', 0.02, 'cot_rep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c225a23-e32b-4088-a384-ff2bb1023e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe26da96-f719-47ad-8519-c01f9b601e64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Risk Management\n",
    "\n",
    "MAX DD PER DAY FROM HIGHEST ACC BALANCE (Including floating PNL) - 3% Models to deal with drawdown limits:\n",
    "\n",
    "Daily forecast of expected returns for the day and adjust accordingly.\n",
    "Constant PNL monitoring algorithm to:\n",
    "Take profits when floating PNL exceeds a certain threshold. For example, if current PNL is 7%, take profit on 5% and keep 2% floating.\n",
    "For the drawdown side, continually hedge / adjust positions based on monte-carlo simulations at -1%, -1.5% and -2%. Close all positions at -2.25% floating, resume positions the next day.\n",
    "\n",
    "Need to:\n",
    "- Record beginning equity at 00:00 gmt+2 to calculate floating position pnl relative to 'beginning equity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "f83508d2-400b-4fa2-b71b-cc171065c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "id": "e4fbe3c8-b000-4d56-b626-caecaec3011f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02222222222222222"
      ]
     },
     "execution_count": 1037,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.03 / 1.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "id": "13bb22c7-e06d-493e-884f-8524291ee731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_position(position):\n",
    "\n",
    "    tick = mt5.symbol_info_tick(position.symbol)\n",
    "\n",
    "    request = {\n",
    "        \"action\" : mt5.TRADE_ACTION_DEAL,\n",
    "        \"position\": position.ticket,\n",
    "        \"symbol\": position.symbol,\n",
    "        \"volume\": position.volume,\n",
    "        \"type\": mt5.ORDER_TYPE_BUY if position.type == 1 else mt5.ORDER_TYPE_SELL,\n",
    "        \"price\": tick.ask if position.type == 1 else tick.bid,\n",
    "        \"deviation\": 20,\n",
    "        \"magic\": 100,\n",
    "        \"comment\": 'Regres Close',\n",
    "        'type_time': mt5.ORDER_TIME_GTC,\n",
    "        'type_filling':mt5.ORDER_FILLING_IOC,\n",
    "\n",
    "        }\n",
    "    result = mt5.order_send(request)\n",
    "    \n",
    "def close_all():\n",
    "    close_positions = []\n",
    "    open_positions = mt5.positions_get()\n",
    "    open_positions\n",
    "    for i in open_positions:\n",
    "        close_positions.append(i)\n",
    "        \n",
    "    for pos in close_positions:\n",
    "        close_position(pos)\n",
    "        \n",
    "class ProfitLossControl:\n",
    "    \n",
    "    def __init__(self, max_daily_dd, profit_threshold, hedging_threshold, starting_equity):\n",
    "        self.max_daily_dd = max_daily_dd\n",
    "        self.profit_threshold = profit_threshold\n",
    "        self.hedging_threshold = hedging_threshold\n",
    "        self.starting_equity = starting_equity\n",
    "    \n",
    "    def start_equity(self):\n",
    "\n",
    "        gmt_plus_2 = pytz.timezone('EET')  # Eastern European Time is typically GMT+2\n",
    "\n",
    "        # Get the current time in UTC and convert it to GMT+2\n",
    "        now_utc = datetime.now(timezone.utc)\n",
    "        time = now_utc.astimezone(gmt_plus_2)\n",
    "        now_gmt_plus_2 = now_utc.astimezone(gmt_plus_2)\n",
    "\n",
    "        # Create a datetime object for the reset time (00:00:01) on the same day in GMT+2\n",
    "        reset_time_today = now_gmt_plus_2.replace(hour=0, minute=0)\n",
    "\n",
    "        if now_gmt_plus_2 != reset_time_today:\n",
    "            info = mt5.account_info()\n",
    "            start_equity = info.equity\n",
    "            print(f\"Starting equity is {start_equity}\")\n",
    "\n",
    "            return start_equity\n",
    "\n",
    "        else:\n",
    "            print(f\"{countdown} remaining until equity reset\")\n",
    "\n",
    "    def day_pnl(self, start_equity): # Return current days equity drawdow / profit as a percentage from the start of the day. \n",
    "        \n",
    "        # Define the GMT+2 timezone\n",
    "        gmt_plus_2 = pytz.timezone('EET')  # Eastern European Time is typically GMT+2\n",
    "\n",
    "        # Get the current time in UTC and convert it to GMT+2\n",
    "        now_utc = datetime.now(timezone.utc)\n",
    "        time = now_utc.astimezone(gmt_plus_2)\n",
    "        now_gmt_plus_2 = now_utc.astimezone(gmt_plus_2)\n",
    "\n",
    "        # Create a datetime object for the reset time (00:00:01) on the same day in GMT+2\n",
    "        reset_time_today = now_gmt_plus_2.replace(hour=0, minute=0, second=1, microsecond=0)\n",
    "\n",
    "        # Calculate the countdown to the reset time\n",
    "        if now_gmt_plus_2 >= reset_time_today:\n",
    "            # If the current time is after the reset time, calculate the countdown to the next day's reset time\n",
    "            reset_time_tomorrow = reset_time_today + timedelta(days=1)\n",
    "            countdown = reset_time_tomorrow - now_gmt_plus_2\n",
    "        else:\n",
    "            # If the current time is before today's reset time, calculate the countdown to today's reset time\n",
    "            countdown = reset_time_today - now_gmt_plus_2\n",
    "        \n",
    "        account_info = mt5.account_info()\n",
    "        equity = account_info.balance + account_info.profit\n",
    "        \n",
    "        day_pnl = equity - start_equity\n",
    "\n",
    "        print(day_pnl)\n",
    "        print(f\"Time until next reset: {countdown}\")\n",
    "        \n",
    "        return int(day_pnl)\n",
    "        \n",
    "    def daily_dd_control(self, day_pnl): \n",
    "        \n",
    "        if day_pnl <= (self.max_daily_dd / 1.35):\n",
    "            print('Closing all positions for the day')\n",
    "            close_all()\n",
    "        else:\n",
    "            print(f'Threshold not reached. Current day pnl percent: {day_pnl}')\n",
    "        \n",
    "obj = ProfitLossControl(-0.03, 0.03, 0.00125, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e7a9c1-e444-4808-aa03-08a9da09da19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "id": "807a5f0f-7fcc-4513-b4ca-674d558ae95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting equity is 5900.91\n"
     ]
    }
   ],
   "source": [
    "start_equity = obj.start_equity() # script will be running at the start of every 9am day // 00:00 gmt +2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "id": "9141960d-ca59-4372-90ed-047c500c47bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5900.91"
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_equity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "id": "cd686773-a72c-441d-ad02-2e8a30a3851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07999999999992724\n",
      "Time until next reset: 11:40:56.570616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1036,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_pnl = obj.day_pnl(start_equity)\n",
    "day_pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4cbb9-3cc7-4c47-8e1c-b7887f2e11db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a47e3-38e7-4473-a260-593618dc27a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e0d46de-8373-4737-bdb3-e01f654b66e1",
   "metadata": {},
   "source": [
    "# FTMO Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceda933-8103-4dc5-8ac3-74e8d897eb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "775d7461-4582-40c2-83d2-48bd99297640",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_val = 0\n",
    "monetary_allocation = {pair: (weight / 100) * account_value for pair, weight in weights.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c4078a-5fc9-4f72-bd4f-561356f67424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
