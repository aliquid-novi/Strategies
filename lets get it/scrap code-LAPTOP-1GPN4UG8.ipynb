{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6ed69b-cb87-49ca-9d95-efa8406f56b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# T1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578eadfc-f6c7-4b45-8e6f-b20581da9b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Optimized Standard Linear Regression\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'head_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m             orders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msell\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(second_pair)\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m \u001b[43moriginal_optimized_linear_regress\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m, in \u001b[0;36moriginal_optimized_linear_regress\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# for pair, features_df in features_dict.items():\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pair, features_df \u001b[38;5;129;01min\u001b[39;00m \u001b[43mhead_features\u001b[49m\u001b[38;5;241m.\u001b[39mitems(): \n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Prepare the data\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     X \u001b[38;5;241m=\u001b[39m features_df\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mvalues[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Exclude last two values for today's and tomorrow's prediction\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     y \u001b[38;5;241m=\u001b[39m features_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Similarly, exclude the last two values \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'head_features' is not defined"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Define the objective function to minimize (MSE)\n",
    "def objective(params, X_test, y_test):\n",
    "    predicted = np.dot(X_test, params)\n",
    "    mse = np.mean((predicted - y_test) ** 2)\n",
    "    return mse\n",
    "\n",
    "og_predictions_today = {} \n",
    "og_predictions_tomorrow = {} \n",
    "\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date_str = current_datetime.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "\n",
    "\n",
    "def original_optimized_linear_regress():\n",
    "    print('Beginning Optimized Standard Linear Regression')\n",
    "    print('')\n",
    "    \n",
    "    # for pair, features_df in features_dict.items():\n",
    "    for pair, features_df in head_features.items(): \n",
    "        # Prepare the data\n",
    "        X = features_df.drop(columns = ['close']).values[:-2]  # Exclude last two values for today's and tomorrow's prediction\n",
    "        y = features_df['close'].values[:-2]  # Similarly, exclude the last two values \n",
    "\n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "        # Optimization with basinhopping\n",
    "        initial_params = np.ones(X_test.shape[1])\n",
    "        result = opt.basinhopping(objective, initial_params, niter=100, stepsize=0.5, minimizer_kwargs={'args': (X_test, y_test)})\n",
    "        optimized_params = result.x\n",
    "        # Using the optimized parameters to make predictions\n",
    "        og_prediction_today = np.dot(features_df.drop(columns = ['close']).iloc[-2, :].values, optimized_params)\n",
    "        og_prediction_tomorrow = np.dot(features_df.drop(columns = ['close']).iloc[-1, :].values, optimized_params)\n",
    "\n",
    "        og_predictions_today[pair] = prediction_today\n",
    "        og_predictions_tomorrow[pair] = prediction_tomorrow\n",
    " \n",
    "    print(f\"Time is {current_date_str}.\")\n",
    "    print('')\n",
    "    \n",
    "    for pair in predictions_today.keys():\n",
    "        first_pair, second_pair = pair  # split the pair into individual currencies\n",
    "        \n",
    "        current_price = round(compute_spread(pair).iloc[-1], 5)\n",
    "        print(f\"For {pair}:\")\n",
    "        print(f\"Today's prediction: {og_predictions_today[pair]:.5f}. Current price: {current_price}\")\n",
    "        \n",
    "        if og_predictions_today[pair] > current_price:  # predicted spread is widening\n",
    "            print(f\"Signal: Sell {first_pair}, Buy {second_pair}\")\n",
    "            orders[\"sell\"].append(first_pair)\n",
    "            orders[\"buy\"].append(second_pair)\n",
    "        elif og_predictions_today[pair] < current_price:  # predicted spread is contracting\n",
    "            print(f\"Signal: Buy {first_pair}, Sell {second_pair}\")\n",
    "            orders[\"buy\"].append(first_pair)\n",
    "            orders[\"sell\"].append(second_pair)\n",
    "        print(\"-----\")\n",
    "\n",
    "original_optimized_linear_regress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7c282-de93-4bb6-aa69-84ac2b69ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChain:\n",
    "    \n",
    "    # Adjust Accordingly \n",
    "    def __init__(self, price_data):\n",
    "        self.data = pd.DataFrame(price_data)\n",
    "        self.states = ['s_bull', 'w_bull', 'doji', 'w_bear', 's_bear']\n",
    "    \n",
    "    def compute_returns(self):\n",
    "        self.data['returns'] = self.data['close'].pct_change()\n",
    "        \n",
    "    # Temporary states, to amend to more sophisticated/specific states\n",
    "    def classify_state(self, ret):\n",
    "        if ret > 0.01:\n",
    "            return 's_bull'\n",
    "        elif ret > 0.003 and ret < 0.01:\n",
    "            return 'w_bull'\n",
    "        elif ret < 0.002 and ret > -0.002:\n",
    "            return 'doji'\n",
    "        elif ret < -0.003 and ret > -0.01:\n",
    "            return 'w_bear'\n",
    "        else:\n",
    "            return 's_bear'\n",
    "        \n",
    "    def classify_states(self):\n",
    "        self.data['state'] = self.data['returns'].apply(self.classify_state)\n",
    "        \n",
    "        \n",
    "    # ChatGPT code testing/creating a matrix to test for ergodicity \n",
    "    def initialize_matrix(self):\n",
    "        self.matrix = np.zeros((len(self.states), len(self.states)))\n",
    "\n",
    "    def construct_transition_matrix(self):\n",
    "        self.initialize_matrix()\n",
    "\n",
    "        for i in range(1, len(self.data['state'])):\n",
    "            curr_state = self.data['state'][i]\n",
    "            prev_state = self.data['state'][i - 1]\n",
    "            curr_index = self.states.index(curr_state)\n",
    "            prev_index = self.states.index(prev_state)\n",
    "\n",
    "            self.matrix[prev_index][curr_index] += 1\n",
    "\n",
    "        # Normalize the matrix to get transition probabilities\n",
    "        self.matrix = self.matrix / self.matrix.sum(axis=1)[:, None]\n",
    "\n",
    "    def is_ergodic(self):\n",
    "        if (self.matrix > 0).all():\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e29feb-313a-4e3e-8dd3-4b103be8610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nearest_neighbor_models = {}  # A dictionary to hold the trained NearestNeighbors models for each pair.\n",
    "\n",
    "def classify_into_state(features, pair):\n",
    "    \"\"\"Classify a given prediction into a state using the NN and DBSCAN.\"\"\"\n",
    "    activation = states_dict[pair][\"activation_model\"].predict(features)\n",
    "    \n",
    "    if pair not in nearest_neighbor_models:\n",
    "        # If NearestNeighbors model hasn't been trained for this pair, train it\n",
    "        train_activations = states_dict[pair][\"activation_model\"].predict(features_dict[pair])\n",
    "        nearest_neighbor_models[pair] = NearestNeighbors(n_neighbors=1).fit(train_activations)\n",
    "\n",
    "    distance, index = nearest_neighbor_models[pair].kneighbors(activation)\n",
    "    \n",
    "    states = states_dict[pair][\"states\"]\n",
    "    state = states[index[0][0]]  # Get the state of the nearest neighbor\n",
    "    \n",
    "    return state\n",
    "\n",
    "def update_transition_matrix(current_state, previous_state, transition_matrix):\n",
    "    \"\"\"Update the given transition matrix based on the transition from previous_state to current_state.\"\"\"\n",
    "    \n",
    "    # Check if previous_state exists in transition_matrix\n",
    "    if previous_state not in transition_matrix:\n",
    "        transition_matrix[previous_state] = {}\n",
    "\n",
    "    # Now update or initialize current_state for the given previous_state\n",
    "    if current_state not in transition_matrix[previous_state]:\n",
    "        transition_matrix[previous_state][current_state] = 0\n",
    "        \n",
    "    transition_matrix[previous_state][current_state] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59faba4-f9f6-4618-aeef-49d8f7fd850a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Regress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc7d3c-eecb-46c8-86b8-2dd2a2e407e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the objective function to minimize (MSE)\n",
    "def objective(params):\n",
    "    predicted = np.dot(X_test, params)\n",
    "    mse = np.mean((predicted - y_test) ** 2)\n",
    "    return mse\n",
    "\n",
    "predictions = {} \n",
    "\n",
    "for pair, features_df in features_dict.items():\n",
    "    \n",
    "    # Prepare the data\n",
    "    X = features_df.values  # Feature values\n",
    "    y = coint_dict[pair].values[59:]  # All values as target\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a linear regression model as an example\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    # print(f\"Mean Squared Error for {pair}: {mse}\")\n",
    "    \n",
    "     # Initialize the parameters for optimization (e.g., as all ones)\n",
    "    initial_params = np.ones(X_test.shape[1])\n",
    "\n",
    "    # Use simulated annealing to optimize the parameters\n",
    "    result = opt.basinhopping(objective, initial_params, niter=100, stepsize=0.5)\n",
    "\n",
    "    # Get the optimized parameters\n",
    "    optimized_params = result.x\n",
    "\n",
    "    # Re-predict using the optimized parameters\n",
    "    y_pred_optimized = np.dot(X_test, optimized_params)\n",
    "\n",
    "    # Save the prediction\n",
    "    predictions[pair] = y_pred_optimized[0]\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred_optimized)\n",
    "    r_squared_optimized = r2_score(y_test, y_pred_optimized)\n",
    "    print(f\"Mean Squared Error for {pair}: {mse}\")\n",
    "    # print(f'R-squared (R^2) for {pair}: {r_squared_optimized}')\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff2577-d844-498c-be0d-229614ac3df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Define the objective function to minimize (MSE)\n",
    "def objective(params):\n",
    "    predicted = np.dot(X_test, params)\n",
    "    mse = np.mean((predicted - y_test) ** 2)\n",
    "    return mse\n",
    "\n",
    "predictions_today = {} \n",
    "predictions_tomorrow = {} \n",
    "change_in_predictions = {}\n",
    "\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date_str = current_datetime.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "\n",
    "def standard_linear_regress():\n",
    "    \n",
    "    print('Beginning Normal Standard Linear Regression')\n",
    "    print('')\n",
    "    for pair, features_df in features_dict.items():\n",
    "        # Prepare the data\n",
    "        X = features_df.values[:-2]  # Exclude last two values for today's and tomorrow's prediction\n",
    "        y = coint_dict[pair].values[59:-2]  # Similarly, exclude the last two values \n",
    "\n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Train a linear regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict for today and tomorrow using the most recent features\n",
    "        prediction_today = model.predict([features_df.iloc[-2, :].values])[0]\n",
    "        prediction_tomorrow = model.predict([features_df.iloc[-1, :].values])[0]\n",
    "\n",
    "        predictions_today[pair] = prediction_today\n",
    "        predictions_tomorrow[pair] = prediction_tomorrow\n",
    "        change_in_predictions[pair] = prediction_today - model.predict([features_df.iloc[-3, :].values])[0]\n",
    "\n",
    "    print(f\"Time is {current_date_str}.\")\n",
    "    for pair in predictions_today.keys():\n",
    "        current_price = round(compute_spread(pair).iloc[-1], 5)\n",
    "        print(f\"For {pair}:\")\n",
    "        print(f\"Today's prediction: {predictions_today[pair]:.5f}. Current price: {current_price}\")\n",
    "        print(f\"Tomorrow's prediction: {predictions_tomorrow[pair]:.5f}\")\n",
    "        print(f\"Change in prediction compared to yesterday: {change_in_predictions[pair]:.5f}\")\n",
    "        print(\"-----\")\n",
    "\n",
    "standard_linear_regress()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cab5fe1-a614-4283-b3b7-1200565894fc",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9e397-255b-4c1a-b213-acff4d7321ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coint_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#TESTING\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pair, data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcoint_dict\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning through \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpair\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.80\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'coint_dict' is not defined"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "for pair, data in coint_dict.items():\n",
    "    print(f\"Running through {pair}\")\n",
    "    split = int(len(data)*0.80)\n",
    "    train_set, test_set = data[:split], data[split:]\n",
    "    \n",
    "    model = ARIMA(train_set, order=(2, 1, 2))\n",
    "    model_fit_0 = model.fit()\n",
    "    \n",
    "    # Convert pandas series to list for rolling window forecast\n",
    "    past = train_set.tolist()\n",
    "\n",
    "    # Empty list for storing predictions\n",
    "    predictions = []\n",
    "\n",
    "    # Keeping only the first 50 data in the test dataset.\n",
    "    # You can run on the whole dataset, but it will take time to run.\n",
    "    test_set = test_set[:50]\n",
    "\n",
    "    # Perform rolling window forecast\n",
    "    for i in range(len(test_set)):\n",
    "        # Define ARIMA model\n",
    "        model = ARIMA(past, order=(2, 1, 2))\n",
    "        # Fit the model\n",
    "        model_fit = model.fit(start_params=model_fit_0.params)\n",
    "        # Make forecast\n",
    "        forecast_results = model_fit.forecast()\n",
    "        pred = forecast_results[0]\n",
    "        # Append prediction\n",
    "        predictions.append(pred)\n",
    "        # Add test value to train set\n",
    "        past.append(test_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6192a-750e-49ed-9322-822b3bdbae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR AND MA\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "for pair, data in coint_dict.items():\n",
    "    print(f\"Running through {pair}\")\n",
    "    split = int(len(data)*0.80)\n",
    "    train_set, test_set = data[:split], data[split:]\n",
    "\n",
    "    # Empty list to store aic/bic score\n",
    "    aic_p = []\n",
    "    bic_p = []\n",
    "\n",
    "    # p values\n",
    "    p = range(1, 6)  # [1,2,3,4,5]\n",
    "\n",
    "    # AIC/BIC score for different values of p\n",
    "    for i in p:\n",
    "        # Define the AR model\n",
    "        model = ARIMA(train_set, order=(i, 1, 0))  \n",
    "        # Fit the model\n",
    "        model_fit = model.fit()\n",
    "        # Get AIC score\n",
    "        aic_temp = model_fit.aic  \n",
    "        # Get BIC score\n",
    "        bic_temp = model_fit.bic\n",
    "        # Append AIC score\n",
    "        aic_p.append(aic_temp) \n",
    "        # Append BIC score\n",
    "        bic_p.append(bic_temp) \n",
    "\n",
    "    # Plot of AIC/BIC score for AR term\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(range(1, 6), aic_p, color='red')\n",
    "    plt.plot(range(1, 6), bic_p)\n",
    "    plt.title('Tuning AR term')\n",
    "    plt.xlabel('p (AR term)')\n",
    "    plt.ylabel('AIC/BIC score')\n",
    "    plt.legend(['AIC score', 'BIC score'])\n",
    "    plt.show() \n",
    "    \n",
    "for pair, data in coint_dict.items():\n",
    "    print(f\"Running through {pair}\")\n",
    "    split = int(len(data)*0.80)\n",
    "    train_set, test_set = data[:split], data[split:]\n",
    "\n",
    "    # Empty list to store AIC/BIC score\n",
    "    aic_q = []\n",
    "    bic_q = []\n",
    "\n",
    "    # q values\n",
    "    q = range(1, 6)\n",
    "\n",
    "    # AIC/BIC score for different values of q\n",
    "    for i in q:\n",
    "        model = ARIMA(train_set, order=(0, 1, i))\n",
    "        model_fit = model.fit()\n",
    "        aic_temp = model_fit.aic\n",
    "        bic_temp = model_fit.bic\n",
    "        aic_q.append(aic_temp)\n",
    "        bic_q.append(bic_temp)\n",
    "\n",
    "    # Plot of AIC/BIC score for MA term\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(range(1, 6), aic_q, color='red')\n",
    "    plt.plot(range(1, 6), bic_q)\n",
    "    plt.title('Tuning MA term')\n",
    "    plt.xlabel('q (MA term)')\n",
    "    plt.ylabel('AIC/BIC score')\n",
    "    plt.legend(['AIC score', 'BIC score'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d665e-8165-4fcd-b602-71ea68354bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Features #\n",
    "\n",
    "def nn_calc_features(pair1, pair2, timeframe, x):\n",
    "    # You'll need to fetch pair1's data in a similar manner as pair2\n",
    "    # Assuming pair1 is fetched similarly as below\n",
    "    pair1 = pd.DataFrame(mt5.copy_rates_from_pos(pair1, timeframe, 0, x))\n",
    "    pair2 = pd.DataFrame(mt5.copy_rates_from_pos(pair2, timeframe, 0, x))\n",
    "    df = pair1[['open', 'high', 'low', 'close']] - pair2[['open', 'high', 'low', 'close']]\n",
    "    \n",
    "    fiften_day_avg = df['close'].rolling(window = 15).mean().round(5)\n",
    "    sixty_day_avg =  df['close'].rolling(window= 60).mean().round(5)\n",
    "    \n",
    "    df['fiften_day_avg'] = fiften_day_avg\n",
    "    df['sixty_day_avg'] = sixty_day_avg\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "nn_features = {}\n",
    "for pair in coint_dict.keys():\n",
    "    pair1 = pair[0]\n",
    "    pair2 = pair[1]\n",
    "    diff = nn_calc_features(pair1, pair2, mt5.TIMEFRAME_D1, 1000)\n",
    "    nn_features[pair] = diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d9539-370b-40e4-88e7-22c83a6db198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChain:\n",
    "    def __init__(self, states, states_dict):\n",
    "        self.states = states\n",
    "        self.transition_matrix = {}\n",
    "        self.classifier = StateClassifier(states_dict)\n",
    "        self.previous_state = None\n",
    "    \n",
    "    def update_transition_matrix(self, current_state, pair):\n",
    "        # Check and initialize the outer dictionary for the pair if needed\n",
    "        if pair not in self.transition_matrix:\n",
    "            self.transition_matrix[pair] = {}\n",
    "\n",
    "        # Check and initialize the second level dictionary for previous_state if needed\n",
    "        if self.previous_state not in self.transition_matrix[pair]:\n",
    "            self.transition_matrix[pair][self.previous_state] = {}\n",
    "\n",
    "        # Check and initialize the innermost dictionary for current_state if needed\n",
    "        if current_state not in self.transition_matrix[pair][self.previous_state]:\n",
    "            self.transition_matrix[pair][self.previous_state][current_state] = 0\n",
    "\n",
    "        # Now you can safely update the count\n",
    "        self.transition_matrix[pair][self.previous_state][current_state] += 1\n",
    "\n",
    "    def classify_samples(self, samples, pair):\n",
    "        return [self.classifier.classify_sample(sample.reshape(1, -1), pair) for sample in samples]\n",
    "\n",
    "    def get_transition_matrix(self):\n",
    "        return self.transition_matrix\n",
    "    \n",
    "    def create_transition_matrix(self, transitions_dict):\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        for pair, transitions in transitions_dict.items():\n",
    "            transition_counts = {}\n",
    "            for from_state, to_states in transitions.items():\n",
    "                for to_state, count in to_states.items():\n",
    "                    if from_state not in transition_counts:\n",
    "                        transition_counts[from_state] = {}\n",
    "                    if to_state not in transition_counts[from_state]:\n",
    "                        transition_counts[from_state][to_state] = 0\n",
    "                    transition_counts[from_state][to_state] += count\n",
    "\n",
    "            probability_matrix = {}\n",
    "            for from_state, to_states in transition_counts.items():\n",
    "                total_transitions = sum(to_states.values())\n",
    "                probability_matrix[from_state] = {to_state: count / total_transitions for to_state, count in to_states.items()}\n",
    "\n",
    "            result[pair] = probability_matrix\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a69cd-7a3a-4f3d-8635-0d38e8597915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the statistics\n",
    "statistics_list = []\n",
    "\n",
    "# Calculate statistics for each group\n",
    "for sub_state, pct_changes in grouped:\n",
    "    mean_values = pct_changes.mean()\n",
    "    mode_values = pct_changes.mode()[0] if not pct_changes.mode().empty else np.nan\n",
    "    std_values = pct_changes.std()\n",
    "    var_values = pct_changes.var()\n",
    "    kurt_values = kurtosis(pct_changes, fisher=True)\n",
    "    \n",
    "    fifteen_MA = pct_changes.rolling(window=15).mean()\n",
    "    forty_five_MA = pct_changes.rolling(window=45).mean()\n",
    "    ratio = fifteen_MA / forty_five_MA\n",
    "    \n",
    "    q1 = ratio.quantile(0.25)\n",
    "    q3 = ratio.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Append statistics to the list\n",
    "    statistics_list.append({\n",
    "        'sub_state': sub_state,\n",
    "        'Mean': mean_values,\n",
    "        'Mode': mode_values,\n",
    "        'STD': std_values,\n",
    "        'VAR': var_values,\n",
    "        'Kurtosis': kurt_values,\n",
    "        'Q1': q1,\n",
    "        'Q3': q3,\n",
    "        'IQR': iqr\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "statistics_df = pd.DataFrame(statistics_list)\n",
    "\n",
    "# Now, split the data\n",
    "X = df[['pct_change', 'RSI']].dropna()\n",
    "y = df['Label'].dropna()\n",
    "sub_states = df['sub_state']\n",
    "\n",
    "X_train, X_test, y_train, y_test, sub_states_train, sub_states_test = train_test_split(\n",
    "    X, y, sub_states, test_size=0.2, random_state=42\n",
    ")\n",
    "# Step 1: Fit a Random Forest model to determine feature importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Step 2: Select important features based on feature importances\n",
    "# (Here, we're arbitrarily choosing to keep the top 10 features)\n",
    "important_feature_indices = np.argsort(importances)[::-1][:10]\n",
    "\n",
    "\n",
    "# Extract the important features from the original data\n",
    "important_feature_names = X_train.columns[important_feature_indices]\n",
    "X_train_important = X_train[important_feature_names]\n",
    "X_test_important = X_test[important_feature_names]\n",
    "\n",
    "# Step 3: Train an SVM model using only important features\n",
    "svm = SVC(kernel='linear', C=1, probability=True)  # Note the `probability=True`\n",
    "svm.fit(X_train_important, y_train)\n",
    "\n",
    "# Evaluate the model (Optional)\n",
    "score = svm.score(X_test_important, y_test)\n",
    "print(f\"SVM model accuracy: {score * 100:.2f}%\")\n",
    "\n",
    "# Step 4: Get confidence scores (continuous values between 0 and 1)\n",
    "confidence_scores = svm.predict_proba(X_test_important)[:, 1]\n",
    "\n",
    "# Normalize the confidence_scores to be between 0 and 1\n",
    "scaler = MinMaxScaler((0, 1))\n",
    "confidence_scores = scaler.fit_transform(confidence_scores.reshape(-1, 1))\n",
    "confidence_scores = confidence_scores.flatten()\n",
    "confidence_scores_df = pd.DataFrame({'Confidence_Score': confidence_scores, 'sub_state': sub_states_test.reset_index(drop=True)})\n",
    "aggregated_scores = confidence_scores_df.groupby('sub_state')['Confidence_Score'].mean()\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "confidence_df = pd.DataFrame({'Confidence_Score': confidence_scores.flatten()})\n",
    "print(confidence_df.head())\n",
    "\n",
    "statistics_df['SVM_Score'] = statistics_df['sub_state'].map(aggregated_scores)\n",
    "\n",
    "# Calculate scores\n",
    "statistics_df['Score'] = statistics_df.apply(calculate_score, axis=1)\n",
    "\n",
    "statistics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f9b1d-231e-4652-85af-8c9665a7d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Transition Matrix Creation \n",
    "        \n",
    "#         prediction_features = features_df.iloc[:-2]\n",
    "#         current_state = classify_into_state(prediction_features, pair)\n",
    "#         print(current_state)\n",
    "        \n",
    "#         if previous_state is not None:\n",
    "#             print('Updating Transition Matrix')\n",
    "#             update_transition_matrix(current_state, previous_state, transition_matrix)\n",
    "#         else:\n",
    "#             print('No Updates')\n",
    "\n",
    "#         previous_state = current_state\n",
    "#       # Normalize the transition matrix \n",
    "#         counter = 0\n",
    "#         for state_from, transitions in transition_matrix.items():\n",
    "#             counter += 1\n",
    "#             print(counter)\n",
    "#             total_transitions = sum(transitions.values())\n",
    "#             if total_transitions > 0:\n",
    "#                 transition_matrix[state_from] = {state_to: count / total_transitions for state_to, count in transitions.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
